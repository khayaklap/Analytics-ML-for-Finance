{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e500af7",
   "metadata": {},
   "source": [
    "# Portfolio Performance & Risk Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094657a0",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1. Analysis of Portfolio Constituents](#section-1)\n",
    "    * [1.1 Daily Returns](#subsection-11)\n",
    "    * [1.2 Normalized Plot of Closing Prices](#subsection-12) \n",
    "    * [1.3 Histogram of Returns](#subsection-13) \n",
    "    * [1.4 Correlation Matrix](#subsection-14) \n",
    "    * [1.5 Normalized Plot of Cumulative Returns](#subsection-15)\n",
    "    * [1.6 Box & Whisker Plot of Returns](#subsection-16) \n",
    "    * [1.7 Dollar Triangle](#subsection-17) \n",
    "    * [1.8 Risk vs Return Trade-off](#subsection-18) \n",
    "    * [1.9 Wins vs Losses](#subsection-19)\n",
    "    * [1.10 Statistical Attributes of Returns](#subsection-110) \n",
    "    * [1.11 What About Drawdowns and Downside Risk?](#subsection-111) \n",
    "    * [1.12 Tail Risk](#subsection-112) \n",
    "    * [1.13 Price Crash Risk](#subsection-113)   \n",
    "    * [1.14 Liquidity Risk](#subsection-114)     \n",
    "$$$$\n",
    "- [2. Construction of Portfolios](#section-2)\n",
    "    * [2.1 Price-Weighted Portfolio (Monthly Rebalancing)](#subsection-21)\n",
    "    * [2.2 Equally-Weighted Portfolio](#subsection-22)\n",
    "    * [2.3 Market-Value-Weighted Portfolio (Monthly Rebalancing)](#subsection-23)\n",
    "    * [2.4 Inverse Volatility Portfolio (Monthly Rebalancing)](#subsection-24)\n",
    "    * [2.5 Random-Weighted Portfolios](#subsection-25)\n",
    "    * [2.6 Global Minimum (Annualized) Variance Portfolio](#subsection-26)\n",
    "    * [2.7 Maximum (Annualized) Sharpe Portfolio](#subsection-27)\n",
    "    * [2.8 Variance Minimization using Scipy](#subsection-28)\n",
    "    * [2.9 Sharpe Maximization using Scipy](#subsection-29)\n",
    "    * [2.10 Portfolio Comparison : Performance against Benchmark](#subsection-210)\n",
    "    * [2.11 Plotting the Efficient Frontier and the Capital Asset Line](#subsection-211)\n",
    "$$$$\n",
    "- [3. Putting It All Together](#section-3)\n",
    "    * [Impact and Magnitude of Diversification](#subsection-31)\n",
    "$$$$\n",
    "- [4. Capital Asset Pricing Model](#section-4)\n",
    "    * [4.1 Market Portfolio and Annualized Covariance](#subsection-41)\n",
    "    * [4.2 Beta, Systematic Risk, and Idiosyncratic Risk](#subsection-42)\n",
    "    * [4.3 Rolling Beta](#subsection-43)\n",
    "    * [4.4 Expected Return and Alpha](#subsection-44)\n",
    "    * [4.5 Security Market Line](#subsection-45)\n",
    "$$$$\n",
    "- [5. Farma-French 5-Factor Model](#section-5)\n",
    "    * [5.1 Factor Data](#subsection-51)\n",
    "    * [5.2 Rolling Correlation With Factors](#subsection-52)\n",
    "    * [5.3 Linear Regression](#subsection-53)\n",
    "    * [5.4 Regression Coefficients : Magnitude of Factors Driving Returns](#subsection-54)\n",
    "    * [5.5 Model Evaluation and Residual Analysis](#subsection-55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb4a84",
   "metadata": {},
   "source": [
    "## Objective \n",
    "\n",
    "This notebook is an introductory guide to understanding stock returns from a statistical perspective, with a focus on the different types of underlying risk. Traditional portfolio construction methods are implemented to show the time-tested effectiveness of diversification in optimizing risk-adjusted returns. Theoretical models for forecasting returns showcase the importance of capturing each asset's relationship with the market (from a linear standpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4159291",
   "metadata": {},
   "source": [
    "## Importing the Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandas_datareader.data as web\n",
    "import numpy as np \n",
    "import yfinance as yf \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from scipy import optimize, stats  \n",
    "from sklearn import linear_model, metrics \n",
    "from statsmodels import tools, regression \n",
    "from statsmodels.graphics import gofplots\n",
    "from statsmodels.stats import stattools\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from matplotlib import ticker \n",
    "np.random.seed(789)\n",
    "# pd.set_option(\"display.max_rows\", None)  \n",
    "# pd.set_option(\"display.max_columns\", None) \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f422c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7647b01",
   "metadata": {},
   "source": [
    "## Loading the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_tech = pd.read_excel(\"file_path/Open.xlsx\", parse_dates = [\"Date\"], index_col = \"Date\")\n",
    "\n",
    "high_tech = pd.read_excel(\"file_path/High.xlsx\", parse_dates = [\"Date\"], index_col = \"Date\")\n",
    "\n",
    "low_tech = pd.read_excel(\"file_path/Low.xlsx\", parse_dates = [\"Date\"], index_col = \"Date\")\n",
    "\n",
    "close_tech = pd.read_excel(\"file_path/Close.xlsx\", parse_dates = [\"Date\"], index_col = \"Date\")\n",
    "\n",
    "adj_close_tech = pd.read_excel(\"file_path/Adj Close.xlsx\", parse_dates = [\"Date\"], index_col = \"Date\")\n",
    "\n",
    "shares_outstd_tech = pd.read_excel(\"file_path/Shares Outstanding.xlsx\", parse_dates = [\"Date\"], index_col = \"Date\")\n",
    "\n",
    "volume_tech = pd.read_excel(\"file_path/Volume.xlsx\", parse_dates = [\"Date\"], index_col = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rate = 0.03\n",
    "start_date = \"2019-12-31\"\n",
    "end_date = \"2024-01-01\" \n",
    "tickers = close_tech.columns.tolist( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82dec0",
   "metadata": {},
   "source": [
    "## Key Assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb29c3d",
   "metadata": {},
   "source": [
    "- **Long-only, stocks-only portfolio for long-term investing**\n",
    "> No short positions are allowed, and all positions are held for at least a month. Portfolios are constructed and rebalanced based on traditional allocation methods, and do not incorporate technical analysis or machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70a240",
   "metadata": {},
   "source": [
    "- **Execution of portfolio rebalancing orders at the closing price of each month's first day (market order)**\n",
    "> In reality, a large block order is broken down into smaller limit orders executed at different offer prices throughout the day. Other order types can be used, such as Immediate or Cancel, All or None, Fill or Kill, pegged, or reserve orders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb30736",
   "metadata": {},
   "source": [
    "- **Absence of trading costs & rebates**\n",
    "> Commissions and rebates are not taken into account. Calculations of effective cost and realized cost require the execution price and the Bid-Ask midpoint at the time of each smaller order. Without these, net returns cannot be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225118a",
   "metadata": {},
   "source": [
    "- **Absence of dividends**\n",
    "> The compounding effect of reinvesting dividends is not taken into account. Without this, total returns cannot be computed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a78ca",
   "metadata": {},
   "source": [
    "- **Use of arithmetic mean return**\n",
    "> Though less used, geometric mean return is more accurate by taking into account serial correlation and period over period compounding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce463624",
   "metadata": {},
   "source": [
    "## 1. Analysis of Portfolio Constituents \n",
    "<a id=\"section-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87183a62",
   "metadata": {},
   "source": [
    "### 1.1 Daily Returns\n",
    "<a id=\"subsection-11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95986a1e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Daily Return}_{t} = \\frac{\\text{Close Price}_{t} - \\text{Close Price}_{t-1}}{\\text{Close Price}_{t-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = adj_close_tech.pct_change(periods = 1).iloc[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24835cf",
   "metadata": {},
   "source": [
    "### 1.2 Normalized Plot of Closing Prices\n",
    "<a id=\"subsection-12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb8664",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Normalized Close Price}_{t} = \\frac{\\text{Close Price}_{t}}{\\text{Close Price}_{0}} \\times 100\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_close_tech = adj_close_tech.iloc[1:]\n",
    "norm_close_tech = adj_close_tech.div(adj_close_tech.iloc[0]).mul(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71073768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normalized_close(norm_close_df, freq = None) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - norm_close_df : DataFrame of normalized closing prices \n",
    "    - freq (optional) : str for resampling frequency -> weekly or monthly or quarterly or annual \n",
    "\n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Draws a line plot of normalized closing prices for each asset\n",
    "    \"\"\"\n",
    "    if freq : \n",
    "        resampling = {\"weekly\" : \"W-Fri\", \"monthly\" : \"BM\", \"quarterly\" : \"BQ\", \"annual\" : \"A\"} \n",
    "        norm_close_df = norm_close_df.resample(resampling[freq]).last( )\n",
    "    \n",
    "    plt.figure(figsize = (12, 7))\n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "    \n",
    "    assets = norm_close_df.columns\n",
    "    for i, asset in enumerate(assets) :\n",
    "        plt.plot(norm_close_df[asset], c = colors[i], label = asset)\n",
    "    \n",
    "    plt.xlabel(xlabel = \"\")\n",
    "    plt.ylabel(ylabel = \"Normalized Close ($)\", fontsize = 12)\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.style.use(\"default\") \n",
    "    plt.show( )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f407c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normalized_close(norm_close_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b4c7d",
   "metadata": {},
   "source": [
    "### 1.3 Histogram of Returns\n",
    "<a id=\"subsection-13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e110ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(returns_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "\n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Draws a histogram and a kernel density estimation plot of daily returns for each asset \n",
    "    - Highlights the minimum return, average return, and maximum return with dotted vertical lines \n",
    "    \"\"\"\n",
    "    assets = returns_df.columns\n",
    "    n_assets = len(returns_df.columns)\n",
    "\n",
    "    if n_assets % 2 != 0 :\n",
    "        n_rows = int(n_assets / 2) + 1\n",
    "    else :\n",
    "        n_rows = int(n_assets / 2)\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, 2, figsize = (12, n_rows * 5))\n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "\n",
    "    for i in range(n_rows) :\n",
    "        for j in range(2) :\n",
    "            if i + i + j <= n_assets - 1 :\n",
    "                asset = assets[i + i + j]\n",
    "                min_ret = returns_df[asset].min( ) * 100\n",
    "                mean_ret = returns_df[asset].mean( ) * 100\n",
    "                max_ret = returns_df[asset].max( ) * 100\n",
    "            \n",
    "                ax[i , j].hist(x = returns_df[asset] * 100, bins = 50, density = True, label = asset, color = colors[i + i + j])\n",
    "                sns.kdeplot(returns_df[asset] * 100, ax = ax[i , j], color = \"black\")\n",
    "                ax[i , j].axvline(x = min_ret, color = \"red\", linestyle = \"--\", label = f\"{round(min_ret, 2)}\")\n",
    "                ax[i , j].axvline(x = mean_ret, color = \"gold\", linestyle = \"--\", label = f\"{round(mean_ret, 2)}\")\n",
    "                ax[i , j].axvline(x = max_ret, color = \"green\", linestyle = \"--\", label = f\"{round(max_ret, 2)}\")\n",
    "                \n",
    "                ax[i , j].set_xlabel(xlabel = \"Daily Return (%)\", fontsize = 12)\n",
    "                if (i + i + j) % 2 == 0 :\n",
    "                    ax[i , j].set_ylabel(\"Density\", fontsize = 12)\n",
    "                else :\n",
    "                    ax[i , j].set_ylabel(\"\")\n",
    "                ax[i , j].legend(fontsize = 10)\n",
    "                \n",
    "    plt.style.use(\"default\") \n",
    "    plt.show( )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27010c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(returns[tickers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df6cf3",
   "metadata": {},
   "source": [
    "### 1.4 Correlation Matrix\n",
    "<a id=\"subsection-14\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fd6c7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{r}_{X, Y} = \\frac{\\sum_{t=1}^{T} (\\text{Return}_{X,t} - \\bar{\\text{Return}_X}) \\cdot (\\text{Return}_{Y,t} - \\bar{\\text{Return}_Y})}{\\sqrt{\\sum_{t=1}^{T} (\\text{Return}_{X,t} - \\bar{\\text{Return}_X})^2 \\cdot \\sum_{t=1}^{T} (\\text{Return}_{Y,t} - \\bar{\\text{Return}_Y})^2}}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\rho = 1 - \\frac{6 \\times \\sum d_i^2}{T \\times (T^2 - 1)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_heatmap(returns_df, method = \"pearson\") :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns\n",
    "    - method : str for the method for computing correlation -> pearson or spearman or kendall\n",
    "\n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Computes the correlation matrix\n",
    "    - Removes repetitive correlation coefficients between the assets\n",
    "    - Applies a green colormap to display the strength of correlation \n",
    "    \"\"\"\n",
    "    corr_mat = returns_df.corr(method = method) \n",
    "    n_assets = len(corr_mat.columns)\n",
    "    \n",
    "    for i in range(n_assets - 1) :\n",
    "        corr_mat.iloc[i , i + 1 :] = np.nan \n",
    "    \n",
    "    plt.figure(figsize = (n_assets, n_assets))\n",
    "    sns.heatmap(corr_mat, annot = True, square = True, vmin = 0.55, vmax = 0.85, cmap = \"Greens\", cbar = False)\n",
    "    sns.set_theme(font_scale = 1) \n",
    "    \n",
    "    plt.xlabel(xlabel = \"\")\n",
    "    plt.ylabel(ylabel = \"\")\n",
    "    plt.style.use(\"default\") \n",
    "    plt.show( )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(returns[tickers], method = \"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254f673",
   "metadata": {},
   "source": [
    "### 1.5 Normalized Plot of Cumulative Returns \n",
    "<a id=\"subsection-15\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd633c9",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\text{Cumulative Return}_{T} = \\left( \\prod_{t=1}^{T} 1 + \\text{Return}_{t} \\right) - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab17b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_returns_stocks = (1 + returns[tickers]).cumprod( ) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_returns_stocks.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1921c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cum_returns(cum_returns_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - cum_returns_df : DataFrame of cumulative returns \n",
    "\n",
    "    Returns :\n",
    "    - None \n",
    "        \n",
    "    Function :\n",
    "    - Draws a line plot of cumulative daily returns for each asset \n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (12, 7))\n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "    \n",
    "    assets = cum_returns_df.columns\n",
    "    for i, asset in enumerate(assets) :\n",
    "        plt.plot(cum_returns_df[asset].mul(100), c = colors[i], label = asset)\n",
    "    \n",
    "    plt.xlabel(xlabel = \"\")\n",
    "    plt.ylabel(ylabel = \"Cumulative Returns (%)\", fontsize = 12)\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aac3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cum_returns(cum_returns_stocks) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c2de7",
   "metadata": {},
   "source": [
    "### 1.6 Box & Whisker Plot of Returns \n",
    "<a id=\"subsection-16\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923121ec",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\text{Maximum} = \\text{Q3} + 1.5 \\times (\\text{Q3} - \\text{Q1}) \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Q3} = \\frac{4 \\times (\\text{N}+1)}{4}\\text{-th ordered value}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Median} = \\frac{(\\text{N}+1)}{2}\\text{-th ordered value} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Q1} = \\frac{(\\text{N}+1)}{4}\\text{-th ordered value}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Minimum} = \\text{Q1} - 1.5 \\times (\\text{Q3} - \\text{Q1})\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b86907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_whisker(returns_df, freq = None) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    - freq (optional) : str for resampling frequency -> weekly or monthly or quarterly or annual \n",
    "\n",
    "    Returns :\n",
    "    - None \n",
    "        \n",
    "    Function :\n",
    "    - Draws a box & whisker plot of returns for each asset (for the specified resampling frequency)\n",
    "    - Plots outlier values of returns for each asset (for the specified resampling frequency)\n",
    "    \"\"\"\n",
    "    n_assets = len(returns_df.columns)\n",
    "    \n",
    "    if freq : \n",
    "        resampling = {\"weekly\" : \"W-Fri\", \"monthly\" : \"BM\", \"quarterly\" : \"BQ\", \"annual\" : \"A\"} \n",
    "        returns_df = returns_df.resample(resampling[freq]).agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1) \n",
    "    \n",
    "    plt.figure(figsize = (n_assets, 8))  \n",
    "    sns.boxplot(data = returns_df.mul(100), palette = \"tab10\")\n",
    "\n",
    "    plt.xlabel(xlabel = \"\")\n",
    "    if freq :\n",
    "        plt.ylabel(ylabel = f\"{freq.title( )} Return (%)\", fontsize = 12)\n",
    "    else :\n",
    "        plt.ylabel(ylabel = \"Daily Return (%)\", fontsize = 12)\n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[tickers], \"weekly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be97608",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[tickers], \"monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca21c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[tickers], \"quarterly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff98f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[tickers], \"annual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca6db0",
   "metadata": {},
   "source": [
    "### 1.7 Dollar Triangle \n",
    "<a id=\"subsection-17\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dollar_triangle(returns_df, initial_balance = 100_000) : \n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns for a particular asset  \n",
    "    - initial_balance : int representing the initial investment in the asset (100_000 by default)\n",
    "    \n",
    "    Returns :\n",
    "    - dollar_triangle_df \n",
    "    \n",
    "    Function :\n",
    "    - Resamples daily returns to annual frequency, and converts to annual log returns \n",
    "    - Computes the dollar value of the initial balance invested in the asset at the beginning of the year and held for n years \n",
    "    \"\"\"\n",
    "    ann_returns = returns_df.resample(\"A\", kind = \"period\").agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1).to_frame( )\n",
    "    ann_log_returns_df = np.log(ann_returns + 1).dropna( )\n",
    "    \n",
    "    max_years = ann_log_returns_df.index.size\n",
    "    windows = list(range(max_years, 0, - 1)) \n",
    "    \n",
    "    for n_years in windows :  \n",
    "        ann_log_returns_df[f\"{n_years} Y\"] = np.exp(n_years * ann_log_returns_df.iloc[: , 0].rolling(n_years).mean( ))\n",
    "\n",
    "    dollar_triangle_df = ann_log_returns_df.drop(columns = ann_log_returns_df.columns[0])\n",
    "    dollar_triangle_df = dollar_triangle_df.mul(initial_balance)  \n",
    "    return dollar_triangle_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dollar_triangle(returns_df, initial_balance = 100_000) :  \n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns for a particular asset  \n",
    "    - initial_balance : int representing the initial investment in the asset (100_000 by default)\n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Computes the dollar triangle for each asset \n",
    "    - Applies a green colormap to compare dollar values over time   \n",
    "    \"\"\"\n",
    "    assets = returns_df.columns\n",
    "    n_assets = len(returns_df.columns)\n",
    "\n",
    "    if n_assets % 2 != 0 :\n",
    "        n_rows = int(n_assets / 2) + 1\n",
    "    else :\n",
    "        n_rows = int(n_assets / 2)\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, 2, figsize = (12, n_rows * 5))\n",
    "\n",
    "    for i in range(n_rows) :\n",
    "        for j in range(2) :\n",
    "            if i + i + j <= n_assets - 1 :\n",
    "                asset = assets[i + i + j]\n",
    "                ticker_dollar_triangle = dollar_triangle(returns_df[asset], initial_balance = 100_000)\n",
    "            \n",
    "                sns.heatmap(data = ticker_dollar_triangle.T, annot = True, square = True, fmt = \".0f\", cmap = \"Greens\", cbar = False, ax = ax[i , j])\n",
    "            \n",
    "                ax[i , j].tick_params(axis = \"y\", left = False, labelleft = False, labelright = True)\n",
    "                ax[i , j].set_xlabel(xlabel = \"Year End\", fontsize = 12)\n",
    "                ax[i , j].set_ylabel(ylabel = f\"Holding Period in {asset}\", fontsize = 12, rotation = 90, va = \"center\", labelpad = - 270)\n",
    "    \n",
    "    plt.style.use(\"default\") \n",
    "    plt.show( )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4aafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dollar_triangle(returns[tickers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b279dee",
   "metadata": {},
   "source": [
    "### 1.8 Risk vs Return Trade-off \n",
    "<a id=\"subsection-18\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4356d8a",
   "metadata": {},
   "source": [
    "**Number of Trading Days** :\n",
    "- Weekly : 5\n",
    "- Monthly : 21\n",
    "- Quarterly : 63\n",
    "- Annual : 252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0857e23",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{\\text{Return}}_{Frequency} = \\frac{1}{T} \\left( \\sum_{t=1}^{T} \\text{Return}_{t} \\right) \\times \\text{Number of Trading Days}_{Frequency}  \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\sigma_{Frequency} = \\sqrt{\\frac{1}{T}\\sum_{t=1}^{T}(\\text{Return}_{t} - \\bar{\\text{Return}})^2} \\times \\sqrt{\\text{Number of Trading Days}_{Frequency}} \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Sharpe Ratio}_{Frequency} = \\frac{\\bar{\\text{Return}}_{Frequency}}{\\sigma_{Frequency}}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_return(returns_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - summary_df : DataFrame displaying the average return, standard deviation, and sharpe ratio for each resampling frequency\n",
    "    \n",
    "    Function :\n",
    "    - Computes the average daily return and average daily standard deviation\n",
    "    - Converts the average daily return and average daily standard deviation to weekly, monthly, quarterly, and annual frequency \n",
    "    \"\"\"      \n",
    "    summary_df = returns_df.agg([\"mean\" , \"std\"]).transpose( )\n",
    "    summary_df.columns = [\"Daily Return\" , \"Daily Volatility\"] \n",
    "    summary_df[\"Daily Sharpe\"] = (summary_df[\"Daily Return\"]).div(summary_df[\"Daily Volatility\"]) \n",
    "    \n",
    "    n_trading_days = {\"weekly\" : 5, \"monthly\" : 21, \"quarterly\" : 63, \"annual\" : 252}\n",
    "    for freq in n_trading_days.keys( ) :\n",
    "        \n",
    "        summary_df[f\"{freq.title( )} Return\"] = summary_df[\"Daily Return\"] * n_trading_days[freq] \n",
    "        summary_df[f\"{freq.title( )} Volatility\"] =  summary_df[\"Daily Volatility\"] * np.sqrt(n_trading_days[freq])  \n",
    "        summary_df[f\"{freq.title( )} Sharpe\"] = (summary_df[f\"{freq.title( )} Return\"]).div(summary_df[f\"{freq.title( )} Volatility\"])\n",
    "        \n",
    "    summary_df.columns = pd.MultiIndex.from_product([[\"Daily\", \"Weekly\", \"Monthly\", \"Quarterly\", \"Annual\"],\n",
    "                                                     [\"Return\", \"Volatility\", \"Sharpe\"]])\n",
    "    return summary_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stocks = risk_return(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stocks.style\\\n",
    "              .format(\"{:.4f}\")\\\n",
    "              .highlight_max(color = \"lightgreen\")\\\n",
    "              .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13, 8))\n",
    "plt.scatter(x = summary_stocks[(\"Annual\", \"Volatility\")].mul(100), y = summary_stocks[(\"Annual\", \"Return\")].mul(100), \n",
    "            s = 40, c = summary_stocks[(\"Annual\", \"Sharpe\")], cmap = \"RdYlGn\")\n",
    "\n",
    "for ticker in summary_stocks.index : \n",
    "    plt.annotate(ticker, xy = (summary_stocks.loc[ticker , (\"Annual\", \"Volatility\")] * 100 - 1,  summary_stocks.loc[ticker , (\"Annual\", \"Return\")] * 100 + 1), size = 10)\n",
    "\n",
    "plt.xlabel(xlabel = \"Annualized Volatility (%)\", fontsize = 12)\n",
    "plt.ylabel(ylabel = \"Annualized Return (%)\", fontsize = 12) \n",
    "plt.xticks(ticks = range(25, 71, 5))\n",
    "plt.grid( )\n",
    "plt.colorbar(label = \"Annual Sharpe\")\n",
    "plt.style.use(\"default\")\n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c170506",
   "metadata": {},
   "source": [
    "### 1.9 Wins vs Losses\n",
    "<a id=\"subsection-19\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c52f81",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Best Return}_{Frequency} = \\max (\\text{Frequency Return}_{t = 1}, \\text{Frequency Return}_{t = 2}, \\text{Frequency Return}_{t = 3}, \\ldots, \\text{Frequency Return}_{t = T}) \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Worst Return}_{Frequency} = \\min (\\text{Frequency Return}_{t = 1}, \\text{Frequency Return}_{t = 2}, \\text{Frequency Return}_{t = 3}, \\ldots, \\text{Frequency Return}_{t = T}) \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e08df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_worst_returns(returns_df):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - returns_df: DataFrame of daily returns\n",
    "\n",
    "    Returns:\n",
    "    - best_worst_df: DataFrame displaying the best and worst returns for each resampling frequency\n",
    "\n",
    "    Function:\n",
    "    - Iteratively resamples daily returns\n",
    "    - For each resampling frequency, computes the maximum return and minimum return for each asset\n",
    "    \"\"\"\n",
    "    best_worst = { }\n",
    "\n",
    "    resampling = {\"D\" : \"Daily\", \"W-Fri\": \"Weekly\", \"BM\": \"Monthly\", \"BQ\": \"Quarterly\", \"A\": \"Annual\"}\n",
    "    for freq in resampling.keys( ):\n",
    "        \n",
    "        if freq == \"D\" :\n",
    "            periodic_returns_df = returns_df.copy( )\n",
    "        else :\n",
    "            periodic_returns_df = returns_df.resample(freq).agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1)\n",
    "        \n",
    "        best_worst[f\"Best {resampling[freq]} Return\"] = np.array(periodic_returns_df.max( ))\n",
    "        best_worst[f\"Worst {resampling[freq]} Return\"] = np.array(periodic_returns_df.min( ))\n",
    "\n",
    "    best_worst_df = pd.DataFrame(data = best_worst, index = returns_df.columns)\n",
    "    best_worst_df.columns = pd.MultiIndex.from_product([[\"Daily\", \"Weekly\", \"Monthly\", \"Quarterly\", \"Annual\"],\n",
    "                                                        [\"Best Return\", \"Worst Return\"]])\n",
    "    return best_worst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_stocks = best_worst_returns(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_stocks.style\\\n",
    "                 .format(\"{:.4f}\")\\\n",
    "                 .highlight_max(color = \"lightgreen\")\\\n",
    "                 .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df0911",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Batting Average}_{Frequency} = \\frac{\\text{Number of Periods with Positive Returns}}{\\text{T}} \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_percentage(returns_df) : \n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - win_df : DataFrame displaying the % of rows with positive returns and the gain-pain ratio for each resampling frequency \n",
    "    \n",
    "    Function :\n",
    "    - Iteratively resamples daily returns \n",
    "    - For each resampling frequency, divides the number of timestamps with return > 0 by the total number of timestamps \n",
    "    \"\"\"\n",
    "    win = { }\n",
    "\n",
    "    resampling = {\"D\" : \"Day\", \"W-Fri\" : \"Week\", \"BM\" : \"Month\", \"BQ\" : \"Quarter\", \"A\" : \"Year\"}\n",
    "    for freq in resampling.keys( ) :\n",
    "        \n",
    "        if freq == \"D\" :\n",
    "            periodic_returns_df = returns_df.copy( )\n",
    "        else :\n",
    "            periodic_returns_df = returns_df.resample(freq).agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1)\n",
    "        \n",
    "        percent_win_list = [  ] \n",
    "        gain_pain_list = [  ]\n",
    "        for asset in returns_df.columns :\n",
    "            asset_periodic_returns = periodic_returns_df[asset].copy( )\n",
    "            \n",
    "            positive_returns = asset_periodic_returns.loc[asset_periodic_returns > 0]\n",
    "            negative_returns = asset_periodic_returns.loc[asset_periodic_returns < 0] \n",
    "            \n",
    "            percent_win_period = (len(positive_returns) / len(asset_periodic_returns)) * 100\n",
    "            percent_win_list.append(percent_win_period)\n",
    "            \n",
    "        win[f\"% of Winning {resampling[freq]}s\"] = percent_win_list\n",
    "\n",
    "    win_df = pd.DataFrame(data = win, index = returns_df.columns)\n",
    "    return win_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_stocks = win_percentage(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3461b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_stocks.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1394c61",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\text{Gain-Pain Ratio}_{Frequency} = \\frac{\\sum_{t=1}^{T} \\max(0, \\text{Return}_{t})}{\\text{|}\\sum_{t=1}^{T} \\min(0, \\text{Return}_{t})\\text{|}}\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "\\text{Pay Off Ratio}_{Frequency} = \\frac{\\frac{1}{T}\\sum_{t=1}^{T} \\text{Return}_{t} > 0}{\\text{|}\\frac{1}{T}\\sum_{t=1}^{T} \\text{Return}_{t} < 0\\text{|}}\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "\\text{Kelly Criterion}_{Frequency} = \\frac{\\text{Pay Off Ratio}_{Frequency} \\times \\text{Batting Average}_{Frequency} - (\\text{1 - Batting Average}_{Frequency})}{\\text{Pay Off Ratio}_{Frequency}}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_loss_ratios(returns_df) :  \n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - win_loss_df : DataFrame displaying the gain-pain ratio, payoff ratio, and kelly criterion for each resampling frequency \n",
    "    \n",
    "    Function :\n",
    "    - Iteratively resamples daily returns \n",
    "    - For each resampling frequency : \n",
    "      - Divides the sum of positive returns by the absolute value of the sum of negative returns \n",
    "      - Divides the average positive return by the absolute value of the average negative return \n",
    "    \"\"\"\n",
    "    win_loss = { }\n",
    "\n",
    "    resampling = {\"D\" : \"Daily\", \"W-Fri\" : \"Weekly\", \"BM\" : \"Monthly\", \"BQ\" : \"Quarterly\", \"A\" : \"Annual\"}\n",
    "    for freq in resampling.keys( ) :\n",
    "        \n",
    "        if freq == \"D\" :\n",
    "            periodic_returns_df = returns_df.copy( )\n",
    "        else :\n",
    "            periodic_returns_df = returns_df.resample(freq).agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1)\n",
    "        \n",
    "        gain_pain_list = [  ]\n",
    "        payoff_list = [  ]\n",
    "        kelly_list = [  ]\n",
    "        for asset in returns_df.columns :\n",
    "            asset_periodic_returns = periodic_returns_df[asset].copy( )\n",
    "            \n",
    "            positive_returns = asset_periodic_returns.loc[asset_periodic_returns > 0]\n",
    "            negative_returns = asset_periodic_returns.loc[asset_periodic_returns < 0] \n",
    "            \n",
    "            gain_pain_period = positive_returns.sum( ) / np.abs(negative_returns.sum( ))\n",
    "            gain_pain_list.append(gain_pain_period)\n",
    "            \n",
    "            payoff_period = positive_returns.mean( ) / np.abs(negative_returns.mean( ))\n",
    "            payoff_list.append(payoff_period)\n",
    "\n",
    "            percent_win_period = len(positive_returns) / len(asset_periodic_returns) \n",
    "            kelly_period = ((payoff_period * percent_win_period - (1 - percent_win_period)) / payoff_period) * 100\n",
    "            kelly_list.append(kelly_period)\n",
    "            \n",
    "        win_loss[f\"{resampling[freq]} Gain-Pain Ratio\"] = gain_pain_list\n",
    "        win_loss[f\"{resampling[freq]} Pay Off Ratio\"] = payoff_list\n",
    "        win_loss[f\"{resampling[freq]} Kelly Criterion (%)\"] = kelly_list\n",
    "\n",
    "    win_loss_df = pd.DataFrame(data = win_loss, index = returns_df.columns)\n",
    "    win_loss_df.columns = pd.MultiIndex.from_product([[\"Daily\", \"Weekly\", \"Monthly\", \"Quarterly\", \"Annual\"],\n",
    "                                                      [\"Gain-Pain Ratio\", \"Pay Off Ratio\", \"Kelly Criterion (%)\"]]) \n",
    "    return win_loss_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_stocks = win_loss_ratios(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_stocks.style\\\n",
    "               .format(\"{:.4f}\")\\\n",
    "               .highlight_max(color = \"lightgreen\")\\\n",
    "               .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streak(returns_df) : \n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - streak_df : DataFrame displaying the longest streak of days with positive / negative returns for each resampling frequency \n",
    "    \n",
    "    Function :\n",
    "    - Iteratively resamples daily returns \n",
    "    - For each resampling frequency, computes the highest number of consecutive days with returns > 0 / returns < 0\n",
    "    \"\"\"\n",
    "    streak = { }\n",
    "\n",
    "    resampling = {\"D\" : \"Day\", \"W-Fri\" : \"Week\", \"BM\" : \"Month\", \"BQ\" : \"Quarter\", \"A\" : \"Year\"}\n",
    "    for freq in resampling.keys( ) :\n",
    "        \n",
    "        if freq == \"D\" :\n",
    "            periodic_returns_df = returns_df.copy( )\n",
    "        else :\n",
    "            periodic_returns_df = returns_df.resample(freq).agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1)\n",
    "        \n",
    "        longest_win_list = [  ]\n",
    "        longest_lose_list = [  ]\n",
    "        for asset in returns_df.columns :\n",
    "            asset_periodic_returns = periodic_returns_df[asset].copy( )\n",
    "            \n",
    "            longest_win_streak = 0\n",
    "            current_win_streak = 0      \n",
    "            for return_value in asset_periodic_returns.values :\n",
    "                if return_value > 0 :\n",
    "                    current_win_streak += 1\n",
    "                    if current_win_streak > longest_win_streak :\n",
    "                        longest_win_streak = current_win_streak\n",
    "                else :\n",
    "                    current_win_streak = 0\n",
    "            longest_win_list.append(longest_win_streak)                   \n",
    "            \n",
    "            longest_lose_streak = 0\n",
    "            current_lose_streak = 0      \n",
    "            for return_value in asset_periodic_returns.values :\n",
    "                if return_value < 0 :\n",
    "                    current_lose_streak += 1\n",
    "                    if current_lose_streak > longest_lose_streak :\n",
    "                        longest_lose_streak = current_lose_streak\n",
    "                else :\n",
    "                    current_lose_streak = 0\n",
    "            longest_lose_list.append(longest_lose_streak) \n",
    "            \n",
    "        streak[f\"Longest Streak of Winning {resampling[freq]}s\"] = longest_win_list\n",
    "        streak[f\"Longest Streak of Losing {resampling[freq]}s\"] = longest_lose_list\n",
    "    \n",
    "    streak_df = pd.DataFrame(data = streak, index = returns_df.columns)   \n",
    "    streak_df = pd.concat(axis = 1, objs = [ streak_df.loc[: , streak_df.columns.str.contains(\"Winning\")] , \n",
    "                                             streak_df.loc[: , streak_df.columns.str.contains(\"Losing\")] ])\n",
    "    streak_df.columns = pd.MultiIndex.from_product([[\"Longest Winning Streak\", \"Longest Losing Streak\"],\n",
    "                                                    [\"Days\", \"Weeks\", \"Months\", \"Quarters\", \"Years\"]]) \n",
    "    return streak_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee92c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_stocks = streak(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355cd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7e7c4",
   "metadata": {},
   "source": [
    "### 1.10 Statistical Attributes of Returns \n",
    "<a id=\"subsection-110\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837405d",
   "metadata": {},
   "source": [
    "***Tests of Normality :***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee2e33",
   "metadata": {},
   "source": [
    "$$ \n",
    "W = \\frac{{(\\sum_{i=1}^{T} a_i x_{(i)})^2}}{{\\sum_{i=1}^{T} (x_i - \\bar{x})^2}} \n",
    "$$ \n",
    "\n",
    "$\\text{Where :}$\n",
    "- $ T \\text{ is the sample size.} $\n",
    "- $ x_{(i)} \\text{ is the \\( i \\)-th order statistic (i.e., the \\( i \\)-th smallest value in the sample).} $\n",
    "- $ \\bar{x} \\text{ is the sample mean.} $\n",
    "- $ a_i \\text{ are constants obtained from the covariance matrix of the order statistics of a normal sample.} $\n",
    "\n",
    "The Shapiro-Wilk test evaluates the null hypothesis that the sample is normally distributed. If the p-value associated with the test statistic $W$ is less than a chosen significance level (e.g., 0.05), the null hypothesis is rejected, suggesting that the sample does not follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6e79b",
   "metadata": {},
   "source": [
    "$$\n",
    "JB = \\frac{T}{6} \\left( S^2 + \\frac{1}{4}(K-3)^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ T \\text{ is the sample size.} $\n",
    "- $ S \\text{ is the sample skewness.} $\n",
    "- $ K \\text{ is the sample kurtosis.} $\n",
    "\n",
    "The Jarque-Bera test evaluates the null hypothesis that the sample is normally distributed. If the p-value associated with the test statistic $JB$ is less than a chosen significance level (e.g., 0.05), the null hypothesis is rejected, suggesting that the sample does not follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90063e3b",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\text{ADF} = \\frac{{\\text{Coefficient Estimate} - 1}}{{\\text{Standard Error}}} \n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\text{Coefficient Estimate is the estimated coefficient of the lagged differenced series in the regression equation.} $\n",
    "- $ \\text{Standard error is the standard error of the coefficient estimate.} $\n",
    "\n",
    "The Augmented Dickey-Fuller test evaluates the null hypothesis that the time series is stationary around a deterministic trend. If the p-value associated with the test statistic $ADF$ is less than a chosen significance level (e.g., 0.05), the null hypothesis is rejected. The time series has a unit root, indicating non-stationarity (or a stochastic trend)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a7848",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{KPSS} = \\frac{{\\sum_{t=1}^{T} (S_t - S_{t-1})^2 / T}}{{(\\sum_{t=1}^{T} S_t^2 / T)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ S_t \\text{ is the cumulative sum of the deviations of the time series from its mean at time \\( t \\).} $\n",
    "- $ T \\text{ is the number of observations in the time series.} $\n",
    "\n",
    "The Kwiatkowski-Phillips-Schmidt-Shin test evaluates the null hypothesis that the time series is stationary around a deterministic trend. If the p-value associated with the test statistic $KPSS$ is less than a chosen significance level (e.g., 0.05), the null hypothesis is rejected. The time series has a unit root, indicating non-stationarity (or a stochastic trend)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b47bd0",
   "metadata": {},
   "source": [
    "***Finding the Best-Fit Distribution :***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443ed90",
   "metadata": {},
   "source": [
    "$$ \n",
    "D = \\max_{i} | F(x_i) - S(x_i) | \n",
    "$$ \n",
    "\n",
    "$\\text{Where :}$\n",
    "- $ F(x) \\text{ is the empirical cumulative distribution function (CDF) of the sample.} $\n",
    "- $ S(x) \\text{ is the theoretical CDF of the specified distribution.} $ \n",
    "- $ x_i \\text{ are the observed data points.} $\n",
    "\n",
    "The Kolmogorov-Smirnov test evaluates the null hypothesis that the sample is drawn from the specified distribution. The closer the p-value associated with the test statistic D is to 1, the more the null hypothesis is rejected (assuming p-value > significance level), suggesting that the sample follows more strongly the specified distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b72f2",
   "metadata": {},
   "source": [
    "***Common Distributions for Fitting Stock Returns :***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f74a74",
   "metadata": {},
   "source": [
    "$$ \n",
    "F(x) = \\frac{1}{2} + \\frac{1}{2} \\cdot \\text{sign}(x - \\mu) \\cdot \\left[1 - \\exp\\left(-\\left(\\frac{|x - \\mu|}{\\beta}\\right)^{\\alpha}\\right)\\right] \n",
    "$$\n",
    "\n",
    "$\\text{Where :}$\n",
    "- $ F(x) \\text{ is the cumulative distribution function at \\( x \\)}. $\n",
    "- $ \\mu \\text{ is the location parameter.} $\n",
    "- $ \\beta \\text{ is the scale parameter.} $\n",
    "- $ \\alpha \\text{ is the shape parameter.} $\n",
    "- $ \\text{sign}(x - \\mu) \\text{ is the sign function, which is 1 if \\( x > \\mu \\), -1 if \\( x < \\mu \\), and 0 if \\( x = \\mu \\).} $\n",
    "- $ |x - \\mu| \\text{ represents the absolute difference between \\( x \\) and \\( \\mu \\).} $\n",
    "\n",
    "The CDF defines the probability that a random variable following the **Generalized Normal Distribution** is less than or equal to a given value $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea55070",
   "metadata": {},
   "source": [
    "$$\n",
    "F(x) = \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{x} \\frac{1}{\\sigma \\sqrt{1 + \\left(\\frac{y - \\mu}{\\gamma}\\right)^2}} e^{-\\frac{1}{2} \\left(\\frac{u}{\\sigma \\sqrt{1 + \\left(\\frac{y - \\mu}{\\gamma}\\right)^2}}\\right)^2} \\, du \n",
    "$$\n",
    "\n",
    "$\\text{Where :}$\n",
    "- $ F(x) \\text{ is the cumulative distribution function at \\( x \\)}. $\n",
    "- $ \\mu \\text{ is the location parameter.} $\n",
    "- $ \\gamma \\text{ is the shape parameter (scale parameter in some formulations).} $\n",
    "- $ \\sigma \\text{ is the scale parameter (shape parameter in some formulations).} $\n",
    "- $ \\text{e is the base of natural logarithms.} $\n",
    "- $ \\pi \\text{ is the mathematical constant pi.} $\n",
    "\n",
    "The CDF defines the probability that a random variable following the **Johnson SU Distribution** is less than or equal to a given value $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59459132",
   "metadata": {},
   "source": [
    "$$ \n",
    "F(x) = \\int_{-\\infty}^{x} \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu \\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu + 1}{2}} \\, dt \n",
    "$$\n",
    "\n",
    "$\\text{Where :}$\n",
    "- $ F(x) \\text{ is the cumulative distribution function at \\( x \\)} $\n",
    "- $ \\nu \\text{ is the degrees of freedom parameter, which affects the shape and tail behavior of the distribution.} $\n",
    "- $ \\Gamma(\\cdot) \\text{ is the gamma function.} $\n",
    "- $ t \\text{ represents the random variable following the Student's t-distribution.} $\n",
    "\n",
    "The CDF defines the probability that a random variable following the **Student's t Distribution** is less than or equal to a given value $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d64396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_attributes(returns_df) : \n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - distribution_df : DataFrame displaying ratios for analyzing the distribution of daily returns \n",
    "    \n",
    "    Function :\n",
    "    - Conducts the Shapiro-Wilik and Jarque-Bera tests of normality and computes the p-values \n",
    "    - Conducts the Augmented Dickey-Fuller, Kwiatkowski-Phillips-Schmidt-Shin tests of stationarity and computes the p-values \n",
    "    - Returns the distribution (and its p-value) that best fits each asset's daily returns amongst 20 different distributions \n",
    "    \"\"\"   \n",
    "    sw_list = [  ]\n",
    "    jb_list = [  ]\n",
    "    adf_list = [  ]\n",
    "    kpss_list = [  ]\n",
    "    best_dist_list = [  ]\n",
    "    best_p_val_list = [  ]\n",
    "    for asset in returns_df.columns : \n",
    "        \n",
    "        p_val_sw = stats.shapiro(returns_df[asset])[1] \n",
    "        p_val_jb = stats.jarque_bera(returns_df[asset])[1]\n",
    "        p_val_adf = adfuller(returns_df[asset])[1]\n",
    "        p_val_kpss = kpss(returns_df[asset])[1]\n",
    "        \n",
    "        sw_list.append(p_val_sw)\n",
    "        jb_list.append(p_val_jb)\n",
    "        adf_list.append(p_val_adf)\n",
    "        kpss_list.append(p_val_kpss)\n",
    "        \n",
    "        dist_names = [\"lognorm\", \"foldnorm\", \"halfnorm\", \"gennorm\", \"exponnorm\", \"powernorm\", \"skewnorm\", \"truncnorm\", \"johnsonsb\", \"johnsonsu\", \n",
    "                      \"beta\", \"burr\", \"cauchy\", \"expon\", \"gamma\", \"genextreme\", \"invgauss\", \"laplace\", \"pareto\", \"t\"]\n",
    "        dist_results = [ ]\n",
    "        for dist_name in dist_names:\n",
    "            \n",
    "            dist = getattr(stats, dist_name)\n",
    "            params = dist.fit(returns_df[asset])\n",
    "            stat, p_val = stats.kstest(returns_df[asset], dist_name, args = params)\n",
    "            dist_results.append((dist_name, p_val))\n",
    "        \n",
    "        best_dist, best_p_val = (max(dist_results, key = lambda item: item[1]))\n",
    "        best_dist_list.append(best_dist)\n",
    "        best_p_val_list.append(best_p_val)\n",
    "        \n",
    "    tests_df = pd.DataFrame({\"Shapiro-Wilk P-Value\" : sw_list, \"Jarque-Bera P-Value\" : jb_list, \n",
    "                             \"Augmented Dickey-Fuller P-Value\" : adf_list, \"Kwiatkowski-Phillips-Schmidt-Shin P-Value\" : kpss_list, \n",
    "                             \"Best-Fit Distribution\" : best_dist_list, \"Best-Fit P-Value\" : best_p_val_list}, \n",
    "                            index = returns_df.columns)\n",
    "    return tests_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792adb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_stocks = distribution_attributes(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e23f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_stocks.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_qq(returns_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "\n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Draws a quantile-quantile plot of daily returns for each asset\n",
    "    \"\"\"\n",
    "    assets = returns_df.columns\n",
    "    n_assets = len(returns_df.columns)\n",
    "    \n",
    "    if n_assets % 2 != 0 :\n",
    "        n_rows = int(n_assets / 2) + 1\n",
    "    else :\n",
    "        n_rows = int(n_assets / 2)\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, 2, figsize = (12, n_rows * 5))\n",
    "\n",
    "    for i in range(n_rows) :\n",
    "        for j in range(2) :\n",
    "            if i + i + j <= n_assets - 1 :\n",
    "                asset = assets[i + i + j]\n",
    "                gofplots.qqplot(returns_df[asset] * 100, line = \"q\", ax = ax[i , j])\n",
    "                                \n",
    "                ax[i , j].set_xlabel(\"Standard Deviations Away From Mean\", fontsize = 12)\n",
    "                if (i + i + j) % 2 == 0 :\n",
    "                    ax[i , j].set_ylabel(\"Daily Returns (%)\", fontsize = 12)\n",
    "                else :\n",
    "                    ax[i , j].set_ylabel(\"\")\n",
    "                ax[i , j].legend([asset], fontsize = 10)\n",
    "    \n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qq(returns[tickers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed2913",
   "metadata": {},
   "source": [
    "### 1.11 What About Drawdowns and Downside Risk? \n",
    "<a id=\"subsection-111\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e849076",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\text{Annualized Downside } \\sigma = \\sqrt{\\frac{1}{T} \\sum_{t=1}^{T} \\min\\left((\\text{Return}_{t} - \\bar{\\text{Return}})^2, 0\\right)} \\times \\sqrt{252} \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Average Drawdown} = \\frac{1}{T} \\sum_{t=1}^{T} \\text{Drawdown}_{t}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Average Drawdown Duration} = \\frac{\\text{Number of Drawdown Days}}{\\text{Number of Drawdowns}} \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Maximum Drawdown} = \\max (\\text{Drawdown}_{t = 1}, \\text{Drawdown}_{t = 2}, \\text{Drawdown}_{t = 3}, \\ldots, \\text{Drawdown}_{t = T}) \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Longest Drawdown Duration} = \\max(\\text{Drawdown Duration}_1, \\text{Drawdown Duration}_2, \\ldots, \\text{Drawdown Duration}_N)\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Sterling Ratio} = \\frac{\\text{Annualized Return} - \\text{Risk-Free Rate}}{\\text{|} \\frac{1}{N} \\sum_{i=1}^{N} \\text{Largest Drawdown}_i\\text{|}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Ulcer Index} = \\sqrt{\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Drawdown}_{t})^2} \\times 100\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Calmar Ratio} = \\frac{\\text{Annualized } \\bar{\\text{Return}}}{\\text{|Maximum Drawdown|}}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Sortino Ratio} = \\frac{\\text{Annualized } \\bar{\\text{Return}} - \\text{Risk-Free Rate}}{\\text{Annualized Downside } \\sigma}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Kappa Ratio} = \\frac{\\text{Annualized } \\bar{\\text{Return}} - \\text{Risk-Free Rate}}{\\text{Annualized Downside } \\sigma + \\text{|Skew|} + \\text{|Kurtosis|}}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeffc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downside(returns_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - downside_df : DataFrame displaying ratios related to downside risk  \n",
    "    \n",
    "    Function :\n",
    "    - Computes the annualized semi-deviation, average drawdown, maximum drawdown, sterling ratio, ulcer index, calmar ratio, sortino ratio, and kappa ratio \n",
    "    \"\"\"\n",
    "    wealth_index = (1 + returns_df).cumprod( )\n",
    "    previous_peaks = wealth_index.cummax( )\n",
    "    drawdowns_df = (wealth_index - previous_peaks) / previous_peaks\n",
    "    \n",
    "    avg_drawdown = np.array(drawdowns_df.mean( )) \n",
    "    max_drawdown = np.array(drawdowns_df.min( )) \n",
    "    \n",
    "    longest_dd_duration_list = [  ]\n",
    "    avg_dd_duration_list = [  ]\n",
    "    sterling_list = [  ]\n",
    "    for asset in drawdowns_df.columns :\n",
    "        \n",
    "        longest_dd_streak = 0\n",
    "        current_dd_streak = 0 \n",
    "        n_drawdowns = 0\n",
    "        for drawdown_value in drawdowns_df[asset].values :\n",
    "            if drawdown_value < 0 :\n",
    "                current_dd_streak += 1\n",
    "                if current_dd_streak > longest_dd_streak :\n",
    "                    longest_dd_streak = current_dd_streak\n",
    "            else :\n",
    "                n_drawdowns += 1 \n",
    "                current_dd_streak = 0\n",
    "        longest_dd_duration_list.append(longest_dd_streak)   \n",
    "        \n",
    "        n_dd_days = len(drawdowns_df[asset].loc[drawdowns_df[asset] < 0]) \n",
    "        avg_dd_duration = n_dd_days / n_drawdowns \n",
    "        avg_dd_duration_list.append(avg_dd_duration)\n",
    "        \n",
    "        ann_return = returns_df[asset].mean( ) * 252 \n",
    "        worst_five_dds = drawdowns_df[asset].nsmallest(5).values\n",
    "        avg_worst_dd = worst_five_dds.mean( )\n",
    "        sterling = (ann_return - rf_rate) / np.abs(avg_worst_dd) \n",
    "        sterling_list.append(sterling)\n",
    "        \n",
    "    mean_squared_drawdowns = np.array(drawdowns_df.pow(2).mean( )) \n",
    "    ulcer_index = np.sqrt(mean_squared_drawdowns) * 100 \n",
    "    \n",
    "    avg_return = np.array(returns_df.mean( ).mul(252)) \n",
    "    calmar = avg_return / np.abs(max_drawdown) \n",
    "    \n",
    "    downside_std_list = [  ]\n",
    "    sortino_list = [  ]\n",
    "    kappa_list = [  ]\n",
    "    for asset in returns_df.columns : \n",
    "        \n",
    "        downside_returns = returns_df[asset].loc[returns_df[asset] < 0]\n",
    "        downside_std = downside_returns.std( ) * np.sqrt(252)\n",
    "        downside_std_list.append(downside_std)\n",
    "        \n",
    "        ann_return = returns_df[asset].mean( ) * 252 \n",
    "        sortino = (ann_return - rf_rate) / downside_std \n",
    "        sortino_list.append(sortino)\n",
    "        \n",
    "        skew = returns_df[asset].skew( )\n",
    "        kurt = returns_df[asset].kurtosis( )\n",
    "        kappa = (ann_return - rf_rate) / (downside_std + np.abs(skew) + np.abs(kurt)) \n",
    "        kappa_list.append(kappa)\n",
    "        \n",
    "    downside_df = pd.DataFrame(data = {\"Annual Downside Standard Deviation\" : downside_std_list, \n",
    "                                       \"Average Drawdown\" : avg_drawdown, \"Average Drawdown Duration (Days)\" : avg_dd_duration_list, \n",
    "                                       \"Maximum Drawdown\" : max_drawdown, \"Longest Drawdown Duration (Days)\" : longest_dd_duration_list, \n",
    "                                       \"Sterling Ratio\" : sterling_list, \"Ulcer Index\" : ulcer_index, \n",
    "                                       \"Calmar Ratio\" : calmar, \"Sortino Ratio\" : sortino_list, \"Kappa Ratio\" : kappa_list}, \n",
    "                               index = returns_df.columns)\n",
    "    return downside_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_stocks = downside(returns[tickers]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06dfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_stocks.style\\\n",
    "               .format(\"{:.4f}\")\\\n",
    "               .highlight_max(color = \"lightgreen\")\\\n",
    "               .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff007aa",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Drawdown}_{t} = \\min \\left( 0 , \\text{Cumulative Return}_{t} - \\text{Previous Peak}_{t} \\right) \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_drawdowns(returns_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - None\n",
    "\n",
    "    Function : \n",
    "    - Computes drawdowns for each asset\n",
    "    - Plots area charts of drawdowns for each asset \n",
    "    \"\"\"\n",
    "    wealth_index = (1 + returns_df).cumprod( )\n",
    "    previous_peaks = wealth_index.cummax( )\n",
    "    drawdowns_df = (wealth_index - previous_peaks) / previous_peaks \n",
    "    \n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "    \n",
    "    ax = drawdowns_df.mul(100).plot(kind = \"area\", alpha = 0.5, figsize = (12 , len(returns_df.columns) * 3), fontsize = 10, \n",
    "                                    subplots = True, sharex = False, color = colors, xlabel = \"\", ylabel = \"Drawdown (%)\") \n",
    "    avg_drawdowns = np.array(drawdowns_df.mean( )) * 100\n",
    "    \n",
    "    for i in range(len(ax)) :\n",
    "        ax[i].axhline(y = avg_drawdowns[i], color = \"red\", linestyle = \"--\", label = str(avg_drawdowns[i].round(2)))\n",
    "        ax[i].legend( )\n",
    "        \n",
    "    plt.style.use(\"default\") \n",
    "    plt.show( ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_drawdowns(returns[tickers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090aade2",
   "metadata": {},
   "source": [
    "### 1.12 Tail Risk\n",
    "<a id=\"subsection-112\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bd193",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\text{Skew} = \\frac{\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return}_{t} - \\bar{\\text{Return}})^3}{\\left(\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return}_{t} - \\bar{\\text{Return}})^2\\right)^{3/2}}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Kurtosis} = \\frac{\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return}_{t} - \\bar{\\text{Return}})^4}{\\left(\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return}_{t} - \\bar{\\text{Return}})^2\\right)^{2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Average Lower Tail Return} = \\frac{1}{T} \\sum_{t=1}^{T} \\text{(Return < % Threshold)}_{t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Average Upper Tail Return} = \\frac{1}{T} \\sum_{t=1}^{T} \\text{(Return > (1 - %) Threshold)}_{t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Tail Ratio} = \\frac{\\text{Average Upper Tail Return}}{\\text{|Average Lower Tail Return|}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Common Sense Ratio} = \\text{Gain-Pain Ratio} \\times \\text{Tail Ratio}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Outlier Win Ratio} = \\frac{\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return} > \\text{99th percentile})_{t}}{\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return > 0})_{t}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Outlier Loss Ratio} = \\frac{\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return} < \\text{1st percentile})_{t}}{\\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return < 0})_{t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc99575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail_attributes(returns_df, threshold = 0.05) : \n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    - threshold : float representing the quantile for lower and upper tail returns -> 0.01 or 0.05 or 0.1 \n",
    "    \n",
    "    Returns :\n",
    "    - tail_df : DataFrame displaying ratios for analyzing the tail characteristics of daily returns \n",
    "    \n",
    "    Function :\n",
    "    - Computes the skew, kurtosis, average lower tail return, average upper tail return, tail ratio, common sense ratio, outlier win ratio, and outlier loss ratio \n",
    "    \"\"\"\n",
    "    tail_df = returns_df.agg([\"skew\" , \"kurt\"]).transpose( )\n",
    "    tail_df.columns = [\"Skew\" , \"Kurtosis\"]  \n",
    "    \n",
    "    avg_lt_return_list = [  ]\n",
    "    avg_ut_return_list = [  ]\n",
    "    tail_list = [  ]\n",
    "    common_sense_list = [  ]\n",
    "    outlier_win_list = [  ]\n",
    "    outlier_loss_list = [  ]\n",
    "    for asset in returns_df.columns :\n",
    "        \n",
    "        lower_tail_threshold = returns_df[asset].quantile(threshold)  \n",
    "        lower_tail_returns = returns_df[asset].loc[returns_df[asset] < lower_tail_threshold]\n",
    "        avg_lt_return = lower_tail_returns.mean( )\n",
    "        \n",
    "        upper_tail_threshold = returns_df[asset].quantile(1 - threshold)\n",
    "        upper_tail_returns = returns_df[asset].loc[returns_df[asset] > upper_tail_threshold]\n",
    "        avg_ut_return = upper_tail_returns.mean( )\n",
    "        \n",
    "        tail = avg_ut_return / np.abs(avg_lt_return)\n",
    "        \n",
    "        positive_returns = returns_df[asset].loc[returns_df[asset] > 0]\n",
    "        negative_returns = returns_df[asset].loc[returns_df[asset] < 0] \n",
    "        gain_pain = positive_returns.sum( ) / np.abs(negative_returns.sum( ))\n",
    "        common_sense = gain_pain * tail \n",
    "        \n",
    "        avg_upper_outlier_return = returns_df[asset].quantile(0.99).mean( )\n",
    "        avg_positive_return = returns_df[asset].loc[returns_df[asset] > 0].mean( )\n",
    "        outlier_win = avg_upper_outlier_return / avg_positive_return\n",
    "        \n",
    "        avg_lower_outlier_return = returns_df[asset].quantile(0.01).mean( )\n",
    "        avg_negative_return = returns_df[asset].loc[returns_df[asset] < 0].mean( )\n",
    "        outlier_loss = avg_lower_outlier_return / avg_negative_return\n",
    "        \n",
    "        avg_lt_return_list.append(avg_lt_return)\n",
    "        avg_ut_return_list.append(avg_ut_return)\n",
    "        tail_list.append(tail)\n",
    "        common_sense_list.append(common_sense)\n",
    "        outlier_win_list.append(outlier_win)\n",
    "        outlier_loss_list.append(outlier_loss)\n",
    "    \n",
    "    tail_df[f\"Average Lower Tail Return (< {int(threshold * 100)}%)\"] = avg_lt_return_list \n",
    "    tail_df[f\"Average Upper Tail Return (> {int((1 - threshold) * 100)}%)\"] = avg_ut_return_list \n",
    "    tail_df[\"Tail Ratio\"] = tail_list\n",
    "    tail_df[\"Common Sense Ratio\"] = common_sense_list\n",
    "    tail_df[\"Outlier Win Ratio (> 99%)\"] = outlier_win_list\n",
    "    tail_df[\"Outlier Loss Ratio (< 1%)\"] = outlier_loss_list \n",
    "\n",
    "    return tail_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33832680",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_stocks = tail_attributes(returns[tickers], threshold = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36369fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_stocks.style\\\n",
    "           .format(\"{:.4f}\")\\\n",
    "           .highlight_max(color = \"lightgreen\")\\\n",
    "           .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa53c3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Historic Value at Risk}_{\\alpha} = \\text{Quantile}_{\\alpha}(\\text{Historical Returns})\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Parametric Value at Risk}_{\\alpha} = \\mu + z_{\\alpha} \\times \\sigma\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Cornish-Fisher Value at Risk}_{\\alpha} = \\mu + \\sigma \\times \\left( z_{\\alpha} + \\frac{1}{6} (z_{\\alpha}^2 - 1) \\times \\text{skewness} + \\frac{1}{24}(z_{\\alpha}^3 - 3z_{\\alpha}) \\times \\text{kurtosis} - \\frac{1}{36}(2z_{\\alpha}^3 - 5z_{\\alpha}) \\times \\text{skewness}^2 \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Conditional Value at Risk}_{\\alpha} = \\frac{1}{1-\\alpha} \\int_{-\\infty}^{\\text{VaR}_{\\alpha}} x \\cdot f(x) \\,dx\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6532734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Value_at_Risk(returns_df, significance = 0.05) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    - significance : float representing the significance level -> 0.01 or 0.05 or 0.1  \n",
    "    \n",
    "    Returns :\n",
    "    - VaR_df : DataFrame displaying ratios related to Value at Risk \n",
    "    \n",
    "    Function :\n",
    "    - Computes the historic VaR, parametric VaR, cornish-fisher VaR, and Conditional VaR at the specified confidence level \n",
    "    \"\"\"\n",
    "    hist_VaR_list = list(np.percentile(returns_df, significance * 100, axis = 0)) \n",
    "    \n",
    "    param_VaR_list = [  ]\n",
    "    cf_VaR_list = [  ]\n",
    "    for asset in returns_df.columns :\n",
    "        \n",
    "        avg_return = returns_df[asset].mean( )\n",
    "        vol = returns_df[asset].std( )\n",
    "        param_VaR = stats.norm.ppf(significance, avg_return, vol)\n",
    "        param_VaR_list.append(param_VaR)\n",
    "        \n",
    "        skew = returns_df[asset].skew( )\n",
    "        kurtosis = returns_df[asset].kurt( )\n",
    "        z_score = stats.norm.ppf(significance)\n",
    "        z_score_cf = z_score + (z_score**2 - 1) * skew / 6 + (z_score**3 - 3 * z_score) * kurtosis / 24 - (2 * z_score**3 - 5 * z_score) * (skew**2) / 36\n",
    "        cf_VaR = avg_return + z_score_cf * vol\n",
    "        cf_VaR_list.append(cf_VaR)\n",
    "        \n",
    "    cond_VaR_list = [  ]\n",
    "    for i in range(len(returns_df.columns)) :\n",
    "        asset = returns_df.columns[i]\n",
    "        hist_VaR = hist_VaR_list[i]\n",
    "        cond_VaR = returns_df[asset].loc[returns_df[asset] <= hist_VaR].mean( )\n",
    "        cond_VaR_list.append(cond_VaR)\n",
    "    \n",
    "    confidence_level = int((1 - significance) * 100)\n",
    "    VaR_df = pd.DataFrame({f\"Historic Value at Risk ({confidence_level}%)\" : hist_VaR_list, \n",
    "                           f\"Parametric Value at Risk ({confidence_level}%)\" : param_VaR_list, \n",
    "                           f\"Cornish-Fisher Value at Risk ({confidence_level}%)\" : cf_VaR_list, \n",
    "                           f\"Conditional Value at Risk ({confidence_level}%)\" : cond_VaR_list}, \n",
    "                           index = returns_df.columns) \n",
    "    return VaR_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ffa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_stocks = Value_at_Risk(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fdc7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_stocks.style\\\n",
    "          .format(\"{:.4f}\")\\\n",
    "          .highlight_max(color = \"lightgreen\")\\\n",
    "          .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Value_at_Risk(VaR_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - VaR_df : DataFrame displaying ratios related to Value at Risk \n",
    "\n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Plots a bar chart of historic, parametric, cornish-fisher, and conditional Value at Risk for each asset \n",
    "    \"\"\"\n",
    "    ax = VaR_df.mul(100).plot(kind = \"bar\", figsize = (12 , 20), fontsize = 10, rot = 0, subplots = True, sharex = False, width = 0.5, \n",
    "                              xlabel = \"\", ylabel = \"%\", title = [\"\", \"\", \"\", \"\"]) \n",
    "    for i in range(len(ax)) :\n",
    "        ax[i].tick_params(axis = \"x\", bottom = False, top = True, labelbottom = False, labeltop = True)\n",
    "        ax[i].axhline(VaR_df.iloc[: , i].mean( ) * 100, linestyle = \"--\", c = \"red\")\n",
    "        \n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5552bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Value_at_Risk(VaR_stocks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation(returns_df, n_simulations = 10_000, n_days = 252) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    - n_simulations : int for the number of random walks to simulate (10_000 by default)\n",
    "    - n_days : int for the number of forward days to forecast (252 by default)\n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Computes Monte Carlo simulations for the future price path of each asset (10_000 by default)\n",
    "    - Plots the 5 simulations with the lowest ending value for each asset \n",
    "    \"\"\"\n",
    "    assets = returns_df.columns\n",
    "    n_assets = len(returns_df.columns)\n",
    "    \n",
    "    if n_assets % 2 != 0 :\n",
    "        n_rows = int(n_assets / 2) + 1\n",
    "    else :\n",
    "        n_rows = int(n_assets / 2)\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, 2, figsize = (12, n_rows * 5))\n",
    "    \n",
    "    for i in range(n_rows) :\n",
    "        for j in range(2) :\n",
    "            if i + i + j <= n_assets - 1 :\n",
    "                asset = assets[i + i + j]\n",
    "                avg_return = returns_df[asset].mean( )\n",
    "                vol = returns_df[asset].std( )\n",
    "                \n",
    "                if asset in tickers :\n",
    "                    value_start = close_tech.iloc[-1, close_tech.columns.get_loc(asset)]\n",
    "                else :\n",
    "                    value_start = 10_000\n",
    "                \n",
    "                simulated_values = { }\n",
    "                for s in range(n_simulations) :\n",
    "                    rand_returns = np.random.normal(avg_return, vol, n_days) + 1\n",
    "                    forecasted_values = value_start * (rand_returns).cumprod( )\n",
    "                    simulated_values[s] = forecasted_values\n",
    "                mc_df = pd.DataFrame(simulated_values)\n",
    "                \n",
    "                bottom_10_paths = mc_df.iloc[-1].nsmallest(5).index.tolist( )\n",
    "                \n",
    "                ax[i , j].plot(list(range(n_days)), mc_df[bottom_10_paths])\n",
    "                ax[i , j].set_xlabel(\"Number of Forward Days\", fontsize = 12)\n",
    "                if (i + i + j) % 2 == 0 :\n",
    "                    ax[i , j].set_ylabel(\"Asset Value ($)\", fontsize = 12)\n",
    "                else :\n",
    "                    ax[i , j].set_ylabel(\"\")\n",
    "                ax[i , j].legend([asset], fontsize = 10)\n",
    "                \n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_simulation(returns[tickers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf61ee7",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x; \\xi, \\mu, \\sigma)_{GEV} = \\frac{1}{\\sigma} \\left[1 + \\xi \\left(\\frac{x - \\mu}{\\sigma}\\right)\\right]^{-1/\\xi - 1} \\exp \\left\\{-\\left[1 + \\xi \\left(\\frac{x - \\mu}{\\sigma}\\right)\\right]^{-1/\\xi}\\right\\}\n",
    "$$ \n",
    "\n",
    "$\\text{Where :}$\n",
    "- $ x \\text{ is the random variable (daily returns).} $\n",
    "- $ \\xi \\text{ is the shape.} $\n",
    "- $ \\mu \\text{ is the location (central tendency).} $ \n",
    "- $ \\sigma \\text{ is the scale parameter (spread).} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad42270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_extreme_value_theory(returns_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Fits a Generalized Extreme Value distribution to the daily returns of each asset \n",
    "    - Plots the GEV probability density curve for each asset \n",
    "    \"\"\"\n",
    "    assets = returns_df.columns\n",
    "    n_assets = len(returns_df.columns)\n",
    "    \n",
    "    if n_assets % 2 != 0 :\n",
    "        n_rows = int(n_assets / 2) + 1\n",
    "    else :\n",
    "        n_rows = int(n_assets / 2)\n",
    "    \n",
    "    fig, ax = plt.subplots(n_rows, 2, figsize = (12, n_rows * 5))\n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "    \n",
    "    for i in range(n_rows) :\n",
    "        for j in range(2) :\n",
    "            if i + i + j <= n_assets - 1 :\n",
    "                asset = assets[i + i + j]\n",
    "                \n",
    "                shape, loc, scale = stats.genextreme.fit(returns_df[asset])\n",
    "                x = np.linspace(stats.genextreme.ppf(0.001, shape, loc = loc, scale = scale),\n",
    "                                stats.genextreme.ppf(0.999, shape, loc = loc, scale = scale), 100)\n",
    "                gev_pdf = stats.genextreme.pdf(x, shape, loc = loc, scale = scale)\n",
    "                \n",
    "                ax[i , j].plot(x, gev_pdf, color = colors[i + i + j])\n",
    "                \n",
    "                ax[i , j].set_xlabel(\"Daily Returns\", fontsize = 12)\n",
    "                if (i + i + j) % 2 == 0 :\n",
    "                    ax[i , j].set_ylabel(\"Probability Density\", fontsize = 12)\n",
    "                else :\n",
    "                    ax[i , j].set_ylabel(\"\")\n",
    "                legend_label = f\"{asset}\\n = {shape:.2f}\\n = {loc:.2f}\\n = {scale:.2f}\"\n",
    "                ax[i , j].legend([legend_label], fontsize = 10)\n",
    "                \n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_extreme_value_theory(returns[tickers])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775767fe",
   "metadata": {},
   "source": [
    "### 1.13 Price Crash Risk \n",
    "<a id=\"subsection-113\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08dd2e6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Annual Down-to-Up } \\sigma = \\log \\left(\\frac{(n_u - 1) \\times \\sum_{t=1}^{T} min(0, \\text{Weekly Return}_t)^2}{(n_d - 1) \\times \\sum_{t=1}^{T} max(0, \\text{Weekly Return}_t)^2}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Annual Negative Coefficient of Skewness} = -\\frac{n \\times (n - 1)^{3/2} \\times \\sum_{t=1}^{T} \\text{Weekly Return}_t^3}{(n - 1) \\times (n - 2) \\times \\left(\\sum_{t=1}^{T} \\text{Weekly Return}_t^2\\right)^{3/2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crash_risk(returns_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    \n",
    "    Returns :\n",
    "    - crash_risk_df : DataFrame displaying ratios related to the risk of price crash\n",
    "    \n",
    "    Function :\n",
    "    - For each year, computes the down-to-up volatility and negative coefficient of skewness \n",
    "    \"\"\"\n",
    "    weekly_returns = returns_df.resample(\"W-Fri\").agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1) \n",
    "    \n",
    "    annual_duvol_list = [  ]\n",
    "    annual_ncskew_list = [  ]\n",
    "    \n",
    "    for asset in weekly_returns.columns :\n",
    "        asset_returns = weekly_returns[asset].copy( )\n",
    "        \n",
    "        for year in weekly_returns.index.year.unique( ) :\n",
    "            returns_in_year = asset_returns.loc[str(year)].copy( )\n",
    "            mean_return = returns_in_year.mean( )\n",
    "            \n",
    "            up_returns = returns_in_year.loc[returns_in_year > mean_return]\n",
    "            down_returns = returns_in_year.loc[returns_in_year < mean_return]\n",
    "            n_up_weeks = len(up_returns)\n",
    "            n_down_weeks = len(down_returns)\n",
    "            sum_squared_up_returns = up_returns.pow(2).sum( )\n",
    "            sum_squared_down_returns = down_returns.pow(2).sum( )\n",
    "            \n",
    "            annual_duvol = np.log(((n_down_weeks - 1) * sum_squared_down_returns) / ((n_up_weeks - 1) * sum_squared_up_returns))\n",
    "            annual_duvol_list.append(annual_duvol)\n",
    "            \n",
    "            n_weeks = len(returns_in_year)\n",
    "            sum_squared_returns = returns_in_year.pow(2).sum( )\n",
    "            sum_cubed_returns = returns_in_year.pow(3).sum( )\n",
    "            \n",
    "            annual_ncskew = - ((n_weeks) * (n_weeks - 1)**(3/2) * sum_cubed_returns) / ((n_weeks - 1) * (n_weeks - 2) * (sum_squared_returns)**(3/2))\n",
    "            annual_ncskew_list.append(annual_ncskew)\n",
    "    \n",
    "    multi_index = pd.MultiIndex.from_product([weekly_returns.columns, weekly_returns.index.year.unique( )], names = [\"Ticker\", \"Year\"])\n",
    "    crash_risk_df = pd.DataFrame({\"Down-to-Up Volatility\" : annual_duvol_list, \"Negative Coefficient of Skewness\" : annual_ncskew_list}, \n",
    "                                 index = multi_index)\n",
    "    crash_risk_df = crash_risk_df.unstack(level = \"Year\")\n",
    "    return crash_risk_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8572428",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_stocks = crash_risk(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_stocks.style\\\n",
    "            .format(\"{:.4f}\")\\\n",
    "            .highlight_max(color = \"lightgreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c424688",
   "metadata": {},
   "source": [
    "### 1.14 Liquidity Risk \n",
    "<a id=\"subsection-114\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f51ad0",
   "metadata": {},
   "source": [
    "## 2. Construction of Portfolios \n",
    "<a id=\"section-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341dff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_weights_rebalancing(weights_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - weights_df : DataFrame of weights of each asset at each timestamp \n",
    "    \n",
    "    Returns :\n",
    "    - daily_weights : DataFrame of daily weights of each asset  \n",
    "        \n",
    "    Function :\n",
    "    - Sets the daily weights of each asset in each month equal to the weights of that month's first day \n",
    "    \"\"\"\n",
    "    daily_weights = weights_df.copy( ) \n",
    "    daily_weights[\"Year\"] = daily_weights.index.year \n",
    "    daily_weights[\"Month\"] = daily_weights.index.month \n",
    "    daily_weights = daily_weights.reset_index( )\n",
    "    \n",
    "    month_start_weights = daily_weights.groupby([\"Year\", \"Month\"]).first( ).reset_index(drop = True)\n",
    "    daily_weights.set_index(\"Date\", inplace = True)\n",
    "    daily_weights.drop(columns = [\"Year\", \"Month\"], inplace = True)\n",
    "    month_start_weights.set_index(\"Date\", inplace = True)\n",
    "    \n",
    "    daily_weights.loc[~ (daily_weights.index.isin(month_start_weights.index)) ] = np.nan \n",
    "    daily_weights.ffill(inplace = True)  \n",
    "    \n",
    "    return daily_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_returns(weights_df, returns_df, benchmarks = [\"^GSPC\", \"^NDXT\", \"IXN\"]) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - weights_df : DataFrame of weights of each asset at each timestamp \n",
    "    - returns_df : DataFrame of daily returns \n",
    "    - benchmarks : list of str representing the benchmarks' tickers \n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "        \n",
    "    Function :\n",
    "    - Plots cumulative returns of the portfolio and benchmark indices \n",
    "    - Plots the annualized 3-month and 6-month rolling volatility of the portfolio \n",
    "    - Plots the weights of each asset over time -> price-weighted / market-value-weighted \n",
    "    - Plots the fixed weights of each asset -> equally-weighted / minimum variance / maximum sharpe \n",
    "    \"\"\"\n",
    "    fig , ax = plt.subplots(figsize = (12 , 24), sharex = False, sharey = False, nrows = 3, ncols = 1)\n",
    "    \n",
    "    for benchmark in benchmarks :\n",
    "        compare = yf.download(tickers = benchmark, start = start_date, end = end_date)\n",
    "        close_compare = compare[\"Adj Close\"].copy( ) \n",
    "        returns_compare = close_compare.pct_change(periods = 1).iloc[1:] \n",
    "        returns_df[benchmark] = returns_compare\n",
    "    \n",
    "    cum_returns = (1 + returns_df).cumprod( ) - 1 \n",
    "    cum_returns.mul(100).plot(fontsize = 10, ax = ax[0])\n",
    "    ax[0].set_xlabel(xlabel = \"\")\n",
    "    ax[0].set_ylabel(ylabel = \"Cumulative Returns (%)\", fontsize = 12) \n",
    "    returns_df.drop(columns = benchmarks, inplace = True)\n",
    "    \n",
    "    monthly_returns = returns_df.resample(\"BM\").agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1)\n",
    "    monthly_returns[\"3 Months\"] = monthly_returns.iloc[: , 0].rolling(3).std( ) * np.sqrt(12)\n",
    "    monthly_returns[\"6 Months\"] = monthly_returns.iloc[: , 0].rolling(6).std( ) * np.sqrt(12)\n",
    "    \n",
    "    ax[1].plot(monthly_returns[\"3 Months\"] * 100, label = \"3 Months\", c = \"red\")\n",
    "    ax[1].plot(monthly_returns[\"6 Months\"] * 100, label = \"6 Months\", c = \"fuchsia\")\n",
    "    ax[1].set_xlabel(xlabel = \"\")\n",
    "    ax[1].set_ylabel(ylabel = \"Annualized Rolling Volatility (%)\", fontsize = 12)\n",
    "    ax[1].legend( )\n",
    "    \n",
    "    asset = returns_df.columns[0]\n",
    "    if asset == \"EWP\" or \"MVP\" in asset or \"MSP\" in asset :\n",
    "        weights_series = weights_df.iloc[0]\n",
    "        weights_series.name = \"Weights\"\n",
    "        \n",
    "        contains_zero = weights_series.round(8).isin([0]).any( )\n",
    "        if contains_zero : \n",
    "            weights_series = weights_series.loc[weights_series.round(8) != 0]\n",
    "        \n",
    "        colors = sns.color_palette(\"Set3\")\n",
    "        ax[2].pie(weights_series, labels = weights_series.index, colors = colors, autopct = \"%.2f%%\", radius = 1.2)\n",
    "        \n",
    "    else :\n",
    "        colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "        for i, asset in enumerate(weights_df.columns) :\n",
    "            ax[2].plot(weights_df[asset].mul(100), color = colors[i], label = asset)\n",
    "\n",
    "        ax[2].set_xlabel(xlabel = \"\")\n",
    "        ax[2].set_ylabel(ylabel = \"Monthly Weights (%)\", fontsize = 12)\n",
    "        ax[2].legend( )\n",
    "    \n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9655f",
   "metadata": {},
   "source": [
    "### 2.1 Price-Weighted Portfolio (Monthly Rebalancing) \n",
    "<a id=\"subsection-21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab04f7d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Weight}_t = \\frac{\\text{Price}_t}{\\sum_{i=1}^{N} \\text{Price}_{i,t}}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{PWP Return}_t = \\sum_{i=1}^{N} \\text{Return}_{i,t} \\times \\text{Weight}_{i,t}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afecbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_PWP = monthly_weights_rebalancing( adj_close_tech.div(adj_close_tech.sum(axis = 1), axis = 0) )\n",
    "returns[\"PWP\"] = returns[tickers].mul(weights_PWP).sum(axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb50985",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_PWP = risk_return(returns[\"PWP\"].to_frame( )) \n",
    "summary_PWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c5388",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_PWP = best_worst_returns(returns[\"PWP\"].to_frame( ))\n",
    "best_worst_PWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_PWP = win_percentage(returns[\"PWP\"].to_frame( ))\n",
    "win_percent_PWP.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_PWP = win_loss_ratios(returns[\"PWP\"].to_frame( )) \n",
    "win_loss_PWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_PWP = streak(returns[\"PWP\"].to_frame( ))\n",
    "win_lose_streak_PWP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_PWP = distribution_attributes(returns[\"PWP\"].to_frame( ))\n",
    "dist_attributes_PWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585339e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_PWP = downside(returns[\"PWP\"].to_frame( )) \n",
    "downside_PWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484afbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_PWP = tail_attributes(returns[\"PWP\"].to_frame( ), threshold = 0.05)\n",
    "tail_PWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_PWP = Value_at_Risk(returns[\"PWP\"].to_frame( ))\n",
    "VaR_PWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01114ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_PWP = crash_risk(returns[\"PWP\"].to_frame( ))\n",
    "crash_PWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c89891",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_returns(weights_PWP, returns[\"PWP\"].to_frame( )) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ec315",
   "metadata": {},
   "source": [
    "### 2.2 Equally-Weighted Portfolio \n",
    "<a id=\"subsection-22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60c243",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{W}_t = \\frac{1}{N}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{EWP Return}_t = \\frac{1}{N} \\times \\sum_{i=1}^{N} \\text{Return}_{i,t}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf47a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_EWP = pd.DataFrame(np.full_like(returns[tickers].values, 1 / len(tickers)), columns = tickers, index = returns.index)\n",
    "returns[\"EWP\"] = returns[tickers].mul(weights_EWP).sum(axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_EWP = risk_return(returns[\"EWP\"].to_frame( ))\n",
    "summary_EWP.round(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03563726",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_EWP = best_worst_returns(returns[\"EWP\"].to_frame( ))\n",
    "best_worst_EWP.round(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_EWP = win_percentage(returns[\"EWP\"].to_frame( )) \n",
    "win_percent_EWP.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fcd8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_EWP = win_loss_ratios(returns[\"EWP\"].to_frame( )) \n",
    "win_loss_EWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f56b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_EWP = streak(returns[\"EWP\"].to_frame( ))\n",
    "win_lose_streak_EWP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_EWP = distribution_attributes(returns[\"EWP\"].to_frame( ))\n",
    "dist_attributes_EWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_EWP = downside(returns[\"EWP\"].to_frame( )) \n",
    "downside_EWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14168837",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_EWP = tail_attributes(returns[\"EWP\"].to_frame( ), threshold = 0.05)\n",
    "tail_EWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fbc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_EWP = Value_at_Risk(returns[\"EWP\"].to_frame( ))\n",
    "VaR_EWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_EWP = crash_risk(returns[\"EWP\"].to_frame( ))\n",
    "crash_EWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_returns(weights_EWP, returns[\"EWP\"].to_frame( ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c413d",
   "metadata": {},
   "source": [
    "### 2.3 Market-Value-Weighted Portfolio (Monthly Rebalancing) \n",
    "<a id=\"subsection-23\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51193df",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Weight}_t = \\frac{\\text{Price}_t \\times \\text{Shares Oustanding}_t}{\\sum_{i=1}^{N} \\text{Price}_{i,t} \\times \\text{Shares Outstanding}_{i,t}}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{MVWP Return}_t = \\sum_{i=1}^{N} \\text{Return}_{i,t} \\times \\text{Weight}_{i,t}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_cap = shares_outstd_tech.iloc[1:].mul(close_tech.iloc[1:]) \n",
    "mkt_cap[\"MVWP\"] = mkt_cap.sum(axis = 1)\n",
    "\n",
    "weights_MVWP = monthly_weights_rebalancing( mkt_cap[tickers].div(mkt_cap[\"MVWP\"], axis = 0) ) \n",
    "returns[\"MVWP\"] = returns[tickers].mul(weights_MVWP).sum(axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ee495",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_MVWP = risk_return(returns[\"MVWP\"].to_frame( ))\n",
    "summary_MVWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_MVWP = best_worst_returns(returns[\"MVWP\"].to_frame( ))\n",
    "best_worst_MVWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_MVWP = win_percentage(returns[\"MVWP\"].to_frame( )) \n",
    "win_percent_MVWP.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4339cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_MVWP = win_loss_ratios(returns[\"MVWP\"].to_frame( )) \n",
    "win_loss_MVWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caca230",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_MVWP = streak(returns[\"MVWP\"].to_frame( ))\n",
    "win_lose_streak_MVWP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_MVWP = distribution_attributes(returns[\"MVWP\"].to_frame( ))\n",
    "dist_attributes_MVWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f84b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_MVWP = downside(returns[\"MVWP\"].to_frame( )) \n",
    "downside_MVWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313715bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_MVWP = tail_attributes(returns[\"MVWP\"].to_frame( ), threshold = 0.05)\n",
    "tail_MVWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_MVWP = Value_at_Risk(returns[\"MVWP\"].to_frame( ))\n",
    "VaR_MVWP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86090bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_MVWP = crash_risk(returns[\"MVWP\"].to_frame( ))\n",
    "crash_MVWP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837299b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_returns(weights_MVWP, returns[\"MVWP\"].to_frame( )) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d39053",
   "metadata": {},
   "source": [
    "### 2.4 Inverse Volatility Portfolio (Monthly Rebalancing) \n",
    "<a id=\"subsection-24\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf935e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Weight}_t = \\frac{\\frac{1}{\\sigma_t}}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i, t}}}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{IVP Return}_t = \\sum_{i=1}^{N} \\text{Return}_{i,t} \\times \\text{Weight}_{i,t}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_vol = returns[tickers].rolling(window = 21).std( )\n",
    "inverse_vol = 1 / rolling_vol\n",
    "\n",
    "weights_IVP = monthly_weights_rebalancing( inverse_vol.div(inverse_vol.sum(axis = 1), axis = 0) )\n",
    "returns[\"IVP\"] = returns[tickers].mul(weights_IVP).sum(axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_IVP = risk_return(returns[\"IVP\"].to_frame( ))\n",
    "summary_IVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5170ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_IVP = best_worst_returns(returns[\"IVP\"].to_frame( ))\n",
    "best_worst_IVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33081a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_IVP = win_percentage(returns[\"IVP\"].to_frame( )) \n",
    "win_percent_IVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ba790",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_IVP = win_loss_ratios(returns[\"IVP\"].to_frame( )) \n",
    "win_loss_IVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_IVP = streak(returns[\"IVP\"].to_frame( ))\n",
    "win_lose_streak_IVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_IVP = distribution_attributes(returns[\"IVP\"].to_frame( ))\n",
    "dist_attributes_IVP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ef88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_IVP = downside(returns[\"IVP\"].to_frame( )) \n",
    "downside_IVP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68935f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_IVP = tail_attributes(returns[\"IVP\"].to_frame( ), threshold = 0.05)\n",
    "tail_IVP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_IVP = Value_at_Risk(returns[\"IVP\"].to_frame( ))\n",
    "VaR_IVP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742093e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_IVP = crash_risk(returns[\"IVP\"].to_frame( ))\n",
    "crash_IVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aebba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_returns(weights_IVP, returns[\"IVP\"].to_frame( )) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ace49f",
   "metadata": {},
   "source": [
    "### 2.5 Random-Weighted Portfolios\n",
    "<a id=\"subsection-25\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9221be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_portfs = 10_000  \n",
    "rand_portfs_weights = np.random.random(num_portfs * len(tickers)).reshape(num_portfs, len(tickers))\n",
    "\n",
    "sum_weights = rand_portfs_weights.sum(axis = 1, keepdims = True)\n",
    "rand_portfs_weights = rand_portfs_weights / sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ea2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_portfs_returns = returns[tickers].dot(rand_portfs_weights.transpose( ))\n",
    "rand_portfs_summary = risk_return(rand_portfs_returns)# .sort_values(by = \"Annual Sharpe\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_portfs_returns.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_portfs_summary.round(4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da467a",
   "metadata": {},
   "source": [
    "### 2.6 Global Minimum (Annualized) Variance Portfolio \n",
    "<a id=\"subsection-26\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_MVP = rand_portfs_weights[rand_portfs_summary[(\"Annual\",  \"Volatility\")].pow(2).idxmin( )]\n",
    "weights_MVP = pd.DataFrame(np.tile(weights_MVP, (len(returns[tickers]), 1)), columns = tickers, index = returns.index)\n",
    "returns[\"MVP\"] = rand_portfs_returns[rand_portfs_summary[(\"Annual\",  \"Volatility\")].pow(2).idxmin( )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce300bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_MVP = risk_return(returns[\"MVP\"].to_frame( ))\n",
    "summary_MVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_MVP = best_worst_returns(returns[\"MVP\"].to_frame( ))\n",
    "best_worst_MVP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676faee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_MVP = win_percentage(returns[\"MVP\"].to_frame( ))  \n",
    "win_percent_MVP.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ab7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_MVP = win_loss_ratios(returns[\"MVP\"].to_frame( )) \n",
    "win_loss_MVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_MVP = streak(returns[\"MVP\"].to_frame( ))\n",
    "win_lose_streak_MVP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844603d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_MVP = distribution_attributes(returns[\"MVP\"].to_frame( ))\n",
    "dist_attributes_MVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_MVP = downside(returns[\"MVP\"].to_frame( ))  \n",
    "downside_MVP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_MVP = tail_attributes(returns[\"MVP\"].to_frame( ), threshold = 0.05)\n",
    "tail_MVP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_MVP = Value_at_Risk(returns[\"MVP\"].to_frame( ))\n",
    "VaR_MVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623957b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_MVP = crash_risk(returns[\"MVP\"].to_frame( ))\n",
    "crash_MVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b716e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_returns(weights_MVP, returns[\"MVP\"].to_frame( ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd968e7",
   "metadata": {},
   "source": [
    "### 2.7 Maximum (Annualized) Sharpe Portfolio \n",
    "<a id=\"subsection-27\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d693bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_MSP = rand_portfs_weights[rand_portfs_summary[(\"Annual\", \"Sharpe\")].idxmax( )]\n",
    "weights_MSP = pd.DataFrame(np.tile(weights_MSP, (len(returns[tickers]), 1)), columns = tickers, index = returns.index)\n",
    "returns[\"MSP\"] = rand_portfs_returns[rand_portfs_summary[(\"Annual\", \"Sharpe\")].idxmax( )]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88adf72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_MSP = risk_return(returns[\"MSP\"].to_frame( )) \n",
    "summary_MSP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aea06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_MSP = best_worst_returns(returns[\"MSP\"].to_frame( )) \n",
    "best_worst_MSP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_MSP = win_percentage(returns[\"MSP\"].to_frame( ))  \n",
    "win_percent_MSP.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_MSP = win_loss_ratios(returns[\"MSP\"].to_frame( )) \n",
    "win_loss_MSP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc45144",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_MSP = streak(returns[\"MSP\"].to_frame( ))\n",
    "win_lose_streak_MSP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1130781",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_MSP = distribution_attributes(returns[\"MSP\"].to_frame( ))\n",
    "dist_attributes_MSP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dcec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_MSP = downside(returns[\"MSP\"].to_frame( ))  \n",
    "downside_MSP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_MSP = tail_attributes(returns[\"MSP\"].to_frame( ), threshold = 0.05)\n",
    "tail_MSP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aeb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_MSP = Value_at_Risk(returns[\"MSP\"].to_frame( ))\n",
    "VaR_MSP.round(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_MSP = crash_risk(returns[\"MSP\"].to_frame( ))\n",
    "crash_MSP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7493e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_returns(weights_MSP, returns[\"MSP\"].to_frame( ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1480f5",
   "metadata": {},
   "source": [
    "### 2.8 Variance Minimization using Scipy\n",
    "<a id=\"subsection-28\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01afc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_returns = returns[tickers].mean( ) * 252 \n",
    "cov_mat = returns[tickers].cov( )  * 252 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4788b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance(weights) :\n",
    "    portf_var = np.dot(weights.T, np.dot(cov_mat, weights)) \n",
    "    return portf_var "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_metric(cov_mat, func) :\n",
    "    constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "    bounds = tuple((0, 1) for asset in range(len(cov_mat)))\n",
    "    initial_weights = np.ones(len(cov_mat)) / len(cov_mat)\n",
    "    \n",
    "    result = optimize.minimize(func, initial_weights, method = \"SLSQP\", bounds = bounds, constraints = constraints)\n",
    "    optimized_weights = result.x\n",
    "    return optimized_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_weights_MVP = optimize_metric(cov_mat, compute_variance) \n",
    "sp_weights_MVP = pd.DataFrame(np.tile(sp_weights_MVP, (len(returns[tickers]), 1)), columns = tickers, index = returns.index)\n",
    "\n",
    "sp_returns_MVP = returns[tickers].mul(sp_weights_MVP).sum(axis = 1)\n",
    "sp_returns_MVP = pd.DataFrame(data = sp_returns_MVP, columns = [\"scipy_MVP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_weights_MVP.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sp_MVP = risk_return(sp_returns_MVP[\"scipy_MVP\"].to_frame( )) \n",
    "MVP_comparison = pd.concat(axis = 0, objs = [summary_MVP, summary_sp_MVP])\n",
    "MVP_comparison.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1976da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_returns(sp_weights_MVP, sp_returns_MVP[\"scipy_MVP\"].to_frame( ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b8ca8",
   "metadata": {},
   "source": [
    "### 2.9 Sharpe Maximization using Scipy\n",
    "<a id=\"subsection-29\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sharpe(weights):\n",
    "    portf_return = np.sum(weights * ann_returns)\n",
    "    portf_std = np.sqrt(np.dot(weights.T, np.dot(cov_mat, weights)))\n",
    "    sharpe = (portf_return - rf_rate) / portf_std\n",
    "    return - sharpe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da546f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_weights_MSP = optimize_metric(cov_mat, compute_sharpe) \n",
    "sp_weights_MSP = pd.DataFrame(np.tile(sp_weights_MSP, (len(returns[tickers]), 1)), columns = tickers, index = returns.index)\n",
    "\n",
    "sp_returns_MSP = returns[tickers].mul(sp_weights_MSP).sum(axis = 1)\n",
    "sp_returns_MSP = pd.DataFrame(data = sp_returns_MSP, columns = [\"scipy_MSP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sp_MSP = risk_return(sp_returns_MSP[\"scipy_MSP\"].to_frame( )) \n",
    "MSP_comparison = pd.concat(axis = 0, objs = [summary_MSP, summary_sp_MSP])\n",
    "MSP_comparison.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5469e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_returns(sp_weights_MSP, sp_returns_MSP[\"scipy_MSP\"].to_frame( ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b990ba3",
   "metadata": {},
   "source": [
    "### 2.10 Portfolio Comparison : Performance against Benchmark \n",
    "<a id=\"subsection-210\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfs = [\"PWP\", \"EWP\", \"MVWP\", \"IVP\", \"MVP\", \"MSP\"]\n",
    "cum_returns_portfs = (1 + returns[portfs]).cumprod( ) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a043f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cum_returns(cum_returns_portfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f828aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(returns[portfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71766ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[portfs])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[portfs], \"weekly\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f90c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[portfs], \"monthly\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48868a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[portfs], \"quarterly\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_whisker(returns[portfs], \"annual\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dollar_triangle(returns[portfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_portfs = pd.concat(objs = [summary_PWP, summary_EWP, summary_MVWP, summary_IVP, summary_MVP, summary_MSP], axis = 0)\n",
    "\n",
    "best_worst_portfs = pd.concat(objs = [best_worst_PWP, best_worst_EWP, best_worst_MVWP, best_worst_IVP, best_worst_MVP, best_worst_MSP], axis = 0)\n",
    "\n",
    "win_percent_portfs = pd.concat(objs = [win_percent_PWP, win_percent_EWP, win_percent_MVWP, win_percent_IVP, win_percent_MVP, win_percent_MSP], axis = 0)\n",
    "\n",
    "win_loss_portfs = pd.concat(objs = [win_loss_PWP, win_loss_EWP, win_loss_MVWP, win_loss_IVP, win_loss_MVP, win_loss_MSP], axis = 0)\n",
    "\n",
    "win_lose_streak_portfs = pd.concat(objs = [win_lose_streak_PWP, win_lose_streak_EWP, win_lose_streak_MVWP, win_lose_streak_IVP, win_lose_streak_MVP, win_lose_streak_MSP], axis = 0)\n",
    "\n",
    "dist_attributes_portfs = pd.concat(objs = [dist_attributes_PWP, dist_attributes_EWP, dist_attributes_MVWP, dist_attributes_IVP, dist_attributes_MVP, dist_attributes_MSP], axis = 0)\n",
    "\n",
    "downside_portfs = pd.concat(objs = [downside_PWP, downside_EWP, downside_MVWP, downside_IVP, downside_MVP, downside_MSP], axis = 0) \n",
    "\n",
    "tail_portfs = pd.concat(objs = [tail_PWP, tail_EWP, tail_MVWP, tail_IVP, tail_MVP, tail_MSP], axis = 0) \n",
    "\n",
    "VaR_portfs = pd.concat(objs = [VaR_PWP, VaR_EWP, VaR_MVWP, VaR_IVP, VaR_MVP, VaR_MSP], axis = 0) \n",
    "\n",
    "crash_portfs = pd.concat(objs = [crash_PWP, crash_EWP, crash_MVWP, crash_IVP, crash_MVP, crash_MSP], axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_portfs.style\\\n",
    "              .format(\"{:.4f}\")\\\n",
    "              .highlight_max(color = \"lightgreen\")\\\n",
    "              .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst_portfs.style\\\n",
    "                 .format(\"{:.4f}\")\\\n",
    "                 .highlight_max(color = \"lightgreen\")\\\n",
    "                 .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent_portfs.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cf27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss_portfs.style\\\n",
    "               .format(\"{:.4f}\")\\\n",
    "               .highlight_max(color = \"lightgreen\")\\\n",
    "               .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030792e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak_portfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a61899",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes_portfs.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2fe7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qq(returns[portfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ff29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside_portfs.style\\\n",
    "               .format(\"{:.4f}\")\\\n",
    "               .highlight_max(color = \"lightgreen\")\\\n",
    "               .highlight_min(color = \"lightcoral\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab74f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_drawdowns(returns[portfs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_portfs.style\\\n",
    "           .format(\"{:.4f}\")\\\n",
    "           .highlight_max(color = \"lightgreen\")\\\n",
    "           .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_portfs.style\\\n",
    "          .format(\"{:.4f}\")\\\n",
    "          .highlight_max(color = \"lightgreen\")\\\n",
    "          .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Value_at_Risk(VaR_portfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b86192",
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_simulation(returns[portfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_extreme_value_theory(returns[portfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_portfs.style\\\n",
    "            .format(\"{:.4f}\")\\\n",
    "            .highlight_max(color = \"lightgreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab55693",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Annualized Active Return}_{portfolio} = \\text{Annualized } \\bar{\\text{Return}}_{portfolio} - \\text{Annualized } \\bar{\\text{Return}}_{benchmark}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Annualized Tracking Error}_{portfolio} = \\sqrt{\\frac{\\sum_{t=1}^{T} (\\text{Return}_{portfolio,t} - \\text{Return}_{benchmark,t})^2}{T}} \\times \\sqrt{252}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Annualized Information Ratio}_{portfolio} = \\frac{\\text{Annualized Active Return}_{portfolio}}{\\text{Annualized Tracking Error}_{portfolio}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Annualized M}^2 \\text{ Ratio}_{portfolio} = \\text{Annualized Sharpe}_{portfolio} \\times \\text{Annualized } \\sigma_{benchmark} + \\text{Risk-Free Rate} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Upside Capture}_{portfolio} = \\frac{\\left( \\prod_{t=1}^{T} 1 + \\max(0, \\text{ Return}_{portfolio,t}) \\right) - 1}{\\left( \\prod_{t=1}^{T} 1 + \\max(0, \\text{ Return}_{benchmark,t}) \\right) - 1} \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Downside Capture}_{portfolio} = \\frac{\\left( \\prod_{t=1}^{T} 1 + \\min(0, \\text{ Return}_{portfolio,t}) \\right) - 1}{\\left( \\prod_{t=1}^{T} 1 + \\min(0, \\text{ Return}_{benchmark,t}) \\right) - 1} \n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{R}^2_{portfolio} = 1 - \\frac{\\sum_{t=1}^{T} (\\text{Return}_{benchmark, t} - \\hat{\\text{Return}}_{benchmark, t})^2}{\\sum_{t=1}^{T} (\\text{Return}_{benchmark, t} - \\bar{\\text{Return}}_{benchmark, t})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_against_benchmark(returns_portfs_df, benchmark = \"^NDXT\") :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_portfs_df : DataFrame of portfolios' daily returns \n",
    "    - benchmark : str for the benchmark ticker (^NDXT by default)\n",
    "    \n",
    "    Returns :\n",
    "    - perf_df : DataFrame displaying ratios of performance relative to a benchmark \n",
    "    \n",
    "    Function :\n",
    "    - Computes the annualized active return, annualized tracking error, annualized information ratio, annualized m-squared ratio, upside cature, downside capture, and r-squared \n",
    "    \"\"\"\n",
    "    compare = yf.download(tickers = benchmark, start = start_date, end = end_date)\n",
    "    close_compare = compare[\"Close\"].copy( ) \n",
    "    benchmark_returns = close_compare.pct_change(periods = 1).iloc[1:] \n",
    "    \n",
    "    active_return_list = [  ]\n",
    "    tracking_error_list = [  ]\n",
    "    information_list = [  ]\n",
    "    m2_list = [  ]\n",
    "    upside_capture_list = [  ]\n",
    "    downside_capture_list = [  ]\n",
    "    r2_list = [  ]\n",
    "    for portf in returns_portfs_df.columns :\n",
    "        \n",
    "        ann_return_portf = returns_portfs_df[portf].mean( ) * 252\n",
    "        ann_return_benchmark = benchmark_returns.mean( ) * 252\n",
    "        active_return = ann_return_portf - ann_return_benchmark\n",
    "        \n",
    "        returns_above_benchmark = returns_portfs_df[portf].sub(benchmark_returns)\n",
    "        tracking_error = returns_above_benchmark.std( ) * np.sqrt(252)\n",
    "        \n",
    "        information = active_return / tracking_error \n",
    "        \n",
    "        ann_sharpe_portf = (ann_return_portf - rf_rate) / (returns_portfs_df[portf].std( ) * np.sqrt(252)) \n",
    "        ann_std_benchmark = benchmark_returns.std( ) * np.sqrt(252)\n",
    "        m2 = ann_sharpe_portf * ann_std_benchmark + rf_rate \n",
    "        \n",
    "        cum_positive_portf = (1 + returns_portfs_df[portf].loc[returns_portfs_df[portf] > 0]).prod( ) - 1\n",
    "        cum_positive_benchmark = (1 + benchmark_returns.loc[benchmark_returns > 0]).prod( ) - 1\n",
    "        upside_capture = cum_positive_portf / cum_positive_benchmark\n",
    "        \n",
    "        cum_negative_portf = (1 + returns_portfs_df[portf].loc[returns_portfs_df[portf] < 0]).prod( ) - 1\n",
    "        cum_negative_benchmark = (1 + benchmark_returns.loc[benchmark_returns < 0]).prod( ) - 1\n",
    "        downside_capture = cum_negative_portf / cum_negative_benchmark\n",
    "        \n",
    "        lin_reg = linear_model.LinearRegression( )\n",
    "        lin_reg.fit(returns_portfs_df[portf].to_frame( ) , benchmark_returns)\n",
    "        Y_pred = lin_reg.predict(returns_portfs_df[portf].to_frame( ))\n",
    "        r2 = metrics.r2_score(benchmark_returns, Y_pred)\n",
    "        \n",
    "        active_return_list.append(active_return)\n",
    "        tracking_error_list.append(tracking_error)\n",
    "        information_list.append(information)\n",
    "        m2_list.append(m2)\n",
    "        upside_capture_list.append(upside_capture)\n",
    "        downside_capture_list.append(downside_capture)\n",
    "        r2_list.append(r2)\n",
    "    \n",
    "    perf_df = pd.DataFrame({\"Annual Active Return\" : active_return_list, \"Annual Tracking Error\" : tracking_error_list, \n",
    "                            \"Annual Information Ratio\" : information_list, \"Annual M2 Ratio\" : m2_list,\n",
    "                            \"Upside Capture\" : upside_capture_list, \"Downside Capture\" : downside_capture_list,  \n",
    "                            \"R-Squared\" : r2_list}, \n",
    "                           index = returns_portfs_df.columns)\n",
    "    print(f\"Benchmark = {benchmark}\")\n",
    "    return perf_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_perf_porfs = perf_against_benchmark(returns[portfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_perf_porfs.style\\\n",
    "                   .format(\"{:.4f}\")\\\n",
    "                   .highlight_max(color = \"lightgreen\")\\\n",
    "                   .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15780455",
   "metadata": {},
   "source": [
    "### 2.11 Plotting the Efficient Frontier and the Capital Asset Line \n",
    "<a id=\"subsection-211\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cee6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12 , 7))  \n",
    "    \n",
    "# plot the randomlyweighted portfolios \n",
    "plt.scatter(x = rand_portfs_summary[(\"Annual\", \"Volatility\")], y = rand_portfs_summary[(\"Annual\", \"Return\")], \n",
    "            c = rand_portfs_summary[(\"Annual\", \"Sharpe\")], cmap = \"RdYlGn\", vmin = 0.5, vmax = 1.4, s = 20)\n",
    "\n",
    "# plot the minimum variance portfolio \n",
    "plt.scatter(x = summary_portfs.loc[\"MVP\" , (\"Annual\", \"Volatility\")], y = summary_portfs.loc[\"MVP\" , (\"Annual\", \"Return\")], s = 30, c = \"blue\") \n",
    "\n",
    "plt.annotate(\"MVP\", size = 8, c = \"blue\", \n",
    "             xy = (summary_portfs.loc[\"MVP\" , (\"Annual\", \"Volatility\")] - 0.03, summary_portfs.loc[\"MVP\" , (\"Annual\", \"Return\")]))\n",
    "\n",
    "# plot the maximum sharpe portfolio \n",
    "plt.scatter(x = summary_portfs.loc[\"MSP\" , (\"Annual\", \"Volatility\")], y = summary_portfs.loc[\"MSP\" , (\"Annual\", \"Return\")],  \n",
    "            c = \"fuchsia\", s = 100, marker = \"*\")\n",
    "\n",
    "plt.annotate(\"MSP\", size = 10, c = \"fuchsia\", \n",
    "             xy = (summary_portfs.loc[\"MSP\" , (\"Annual\", \"Volatility\")] - 0.01, summary_portfs.loc[\"MSP\" , (\"Annual\", \"Return\")] + 0.02))\n",
    "\n",
    "# plot the priceweighted portfolio\n",
    "plt.scatter(x = summary_portfs.loc[\"PWP\" , (\"Annual\", \"Volatility\")], y = summary_portfs.loc[\"PWP\" , (\"Annual\", \"Return\")], s = 30, c = \"blue\")\n",
    "\n",
    "plt.annotate(\"PWP\", size = 8, c = \"blue\", \n",
    "             xy = (summary_portfs.loc[\"PWP\" , (\"Annual\", \"Volatility\")] + 0.01, summary_portfs.loc[\"PWP\" , (\"Annual\", \"Return\")]))\n",
    "\n",
    "# plot the equallyweighted portfolio\n",
    "plt.scatter(x = summary_portfs.loc[\"EWP\" , (\"Annual\", \"Volatility\")], y = summary_portfs.loc[\"EWP\" , (\"Annual\", \"Return\")], s = 30, c = \"blue\")\n",
    "\n",
    "plt.annotate(\"EWP\", size = 8, c = \"blue\", \n",
    "             xy = (summary_portfs.loc[\"EWP\" , (\"Annual\", \"Volatility\")] - 0.01, summary_portfs.loc[\"EWP\" , (\"Annual\", \"Return\")] + 0.015))\n",
    "\n",
    "# plot the marketvalueweighted portfolio \n",
    "plt.scatter(x = summary_portfs.loc[\"MVWP\" , (\"Annual\", \"Volatility\")], y = summary_portfs.loc[\"MVWP\" , (\"Annual\", \"Return\")], c = \"blue\" , s = 30)\n",
    "\n",
    "plt.annotate(\"MVWP\", size = 8, c = \"blue\", \n",
    "             xy = (summary_portfs.loc[\"MVWP\", (\"Annual\", \"Volatility\")] - 0.04, summary_portfs.loc[\"MVWP\" , (\"Annual\", \"Return\")]))  \n",
    "\n",
    "# plot the inverse volatility portfolio \n",
    "plt.scatter(x = summary_portfs.loc[\"IVP\" , (\"Annual\", \"Volatility\")], y = summary_portfs.loc[\"IVP\" , (\"Annual\", \"Return\")], c = \"blue\" , s = 30)\n",
    "\n",
    "plt.annotate(\"IVP\", size = 8, c = \"blue\", \n",
    "             xy = (summary_portfs.loc[\"IVP\", (\"Annual\", \"Volatility\")] - 0.02, summary_portfs.loc[\"IVP\" , (\"Annual\", \"Return\")] + 0.015))  \n",
    "\n",
    "# plot the individual stocks in the portfolio \n",
    "plt.scatter(x = summary_stocks[(\"Annual\", \"Volatility\")], y = summary_stocks[(\"Annual\", \"Return\")], s = 30, c = \"black\")\n",
    "\n",
    "for ticker in tickers : \n",
    "    plt.annotate(ticker, xy = (summary_stocks.loc[ticker , (\"Annual\", \"Volatility\")] - 0.01, summary_stocks.loc[ticker , (\"Annual\", \"Return\")] + 0.015), size = 8)  \n",
    "\n",
    "# plot the risk free asset and the capital market line \n",
    "plt.scatter(x = pd.Series(data = 0).values, y = rf_rate, s = 30, c = \"fuchsia\")\n",
    "plt.annotate(\"risk-free asset\", size = 10, c = \"fuchsia\", xy = (rf_rate, rf_rate))\n",
    "\n",
    "plt.plot([0, summary_portfs.loc[\"MSP\" , (\"Annual\", \"Volatility\")]], [rf_rate, summary_portfs.loc[\"MSP\" , (\"Annual\", \"Return\")]], c = \"fuchsia\", marker = \"o\", linestyle = \"-\")\n",
    "plt.annotate(\"CAL\", size = 10, c = \"fuchsia\", xy = (0.1, 0.2))\n",
    "\n",
    "plt.xticks(ticks = np.array(range(0, 71, 5)) / 100)\n",
    "plt.xlabel(xlabel = \"Annualized Volatility\", fontsize = 12)\n",
    "plt.ylabel(ylabel = \"Annualized Return\", fontsize = 12) \n",
    "plt.grid( )\n",
    "plt.show( )   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a2c9e",
   "metadata": {},
   "source": [
    "## 3. Putting It All Together \n",
    "<a id=\"section-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f106e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.concat(objs = [summary_stocks, summary_portfs], axis = 0) \n",
    "\n",
    "best_worst = pd.concat(objs = [best_worst_stocks, best_worst_portfs], axis = 0) \n",
    "\n",
    "win_percent = pd.concat(objs = [win_percent_stocks, win_percent_portfs], axis = 0)\n",
    "\n",
    "win_loss = pd.concat(objs = [win_loss_stocks, win_loss_portfs], axis = 0)\n",
    "\n",
    "win_lose_streak = pd.concat(objs = [win_lose_streak_stocks, win_lose_streak_portfs], axis = 0)\n",
    "\n",
    "dist_attributes = pd.concat(objs = [dist_attributes_stocks, dist_attributes_portfs], axis = 0)\n",
    "\n",
    "downside = pd.concat(objs = [downside_stocks, downside_portfs], axis = 0) \n",
    "\n",
    "tail = pd.concat(objs = [tail_stocks, tail_portfs], axis = 0) \n",
    "\n",
    "VaR = pd.concat(objs = [VaR_stocks, VaR_portfs], axis = 0) \n",
    "\n",
    "crash = pd.concat(objs = [crash_stocks, crash_portfs], axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeab98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.style\\\n",
    "       .format(\"{:.4f}\")\\\n",
    "       .highlight_max(color = \"lightgreen\")\\\n",
    "       .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184cbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_worst.style\\\n",
    "          .format(\"{:.4f}\")\\\n",
    "          .highlight_max(color = \"lightgreen\")\\\n",
    "          .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_percent.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_loss.style\\\n",
    "        .format(\"{:.4f}\")\\\n",
    "        .highlight_max(color = \"lightgreen\")\\\n",
    "        .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_lose_streak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b59bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_attributes.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ed72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "downside.style\\\n",
    "        .format(\"{:.4f}\")\\\n",
    "        .highlight_max(color = \"lightgreen\")\\\n",
    "        .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce949f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail.style\\\n",
    "    .format(\"{:.4f}\")\\\n",
    "    .highlight_max(color = \"lightgreen\")\\\n",
    "    .highlight_min(color = \"lightcoral\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR.style\\\n",
    "   .format(\"{:.4f}\")\\\n",
    "   .highlight_max(color = \"lightgreen\")\\\n",
    "   .highlight_min(color = \"lightcoral\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash.style\\\n",
    "     .format(\"{:.4f}\")\\\n",
    "     .highlight_max(color = \"lightgreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf9026",
   "metadata": {},
   "source": [
    "### Impact and Magnitude of Diversification \n",
    "<a id=\"subsection-31\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c339807",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Diversification Ratio}_{t} = \\frac{\\sigma_{portfolio,t}}{\\sum_{i=1}^{N} \\sigma_{i,t} \\times \\text{Weight}_{i,t}} - 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Herfindahl-Hirschman Index}_{t} = {\\sum_{i=1}^{N} (\\text{Weight}_{i,t})^2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c52ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_portf_diversification(returns_tickers, weights_portf, returns_portf) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_tickers : DataFrame of daily returns for assets in the portfolio \n",
    "    - weights_portf : DataFrame of monthly weights for assets in the portfolio\n",
    "    - returns_portf : DataFrame of daily returns for the portfolio \n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "        \n",
    "    Function :\n",
    "    - Draws a line plot of the monthly weighted sum of assets' volatilities \n",
    "    - Draws a line plot of the monthly volatility of the portfolio\n",
    "    - Draws a line plot of the % difference between portfolio's volatility and assets' volatilities \n",
    "    - Draws a line plot of the sum of squared monthly weights \n",
    "    \"\"\"\n",
    "    monthly_std_tickers = returns_tickers.resample(\"BM\").std( )\n",
    "    monthly_weights = weights_portf.resample(\"BM\").last( )\n",
    "    weighted_sum_std = monthly_std_tickers.mul(monthly_weights).sum(axis = 1)\n",
    "    \n",
    "    portf = returns_portf.name \n",
    "    monthly_std_portf = returns_portf.resample(\"BM\").std( )\n",
    "    \n",
    "    diversif_ratio = - (monthly_std_portf.sub(weighted_sum_std)).div(weighted_sum_std)\n",
    "    herf_hirsch_index = monthly_weights.pow(2).sum(axis = 1)\n",
    "    \n",
    "    fig , ax = plt.subplots(figsize = (11.5 , 14), sharex = False, sharey = False, nrows = 2, ncols = 1)\n",
    "    \n",
    "    ax[0].plot(weighted_sum_std.mul(100), label = \"Weighted Sum of Assets's Volatilities\", c = \"red\")\n",
    "    ax[0].plot(monthly_std_portf.mul(100), label = f\"{portf} Volatility\", c = \"orange\")\n",
    "    ax[0].set_ylabel(\"Monthly (%)\", fontsize = 11)\n",
    "    ax[0].legend( )\n",
    "    \n",
    "    ax[1].plot(diversif_ratio.mul(100), linestyle = \"dotted\", marker = \"o\", color = \"tab:green\", label = \"Diversification Ratio\")\n",
    "    ax[1].set_ylabel(\"Monthly (%)\", fontsize = 11)\n",
    "    \n",
    "    ax2 = ax[1].twinx( )\n",
    "    ax2.plot(herf_hirsch_index.mul(100), linestyle = \"dashed\", marker = \"s\", color = \"tab:blue\", label = \"Herfindahl-Hirschman Index\")\n",
    "    ax2.set_ylabel(\"Monthly (%)\", fontsize = 11)\n",
    "    \n",
    "    lines1, labels1 = ax[1].get_legend_handles_labels( )\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels( )\n",
    "    ax[1].legend(lines1 + lines2, labels1 + labels2, loc = \"best\")\n",
    "    \n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff569a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portf_diversification(returns[tickers], weights_PWP, returns[\"PWP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84116df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portf_diversification(returns[tickers], weights_EWP, returns[\"EWP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d79cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portf_diversification(returns[tickers], weights_MVWP, returns[\"MVWP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portf_diversification(returns[tickers], weights_IVP, returns[\"IVP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfb1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portf_diversification(returns[tickers], weights_MVP, returns[\"MVP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d90609",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_portf_diversification(returns[tickers], weights_MSP, returns[\"MSP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d004e",
   "metadata": {},
   "source": [
    "## 4. Capital Asset Pricing Model \n",
    "<a id=\"section-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cea7c",
   "metadata": {},
   "source": [
    "### 4.1 Market Portfolio and Annualized Covariance\n",
    "<a id=\"subsection-41\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab2930",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Annualized Cov}(\\text{Return}_X, \\text{Return}_Y) = \\left( \\frac{1}{T-1} \\sum_{t=1}^{T} (\\text{Return}_{X,t} - \\bar{\\text{Return}}_{X}) \\times (\\text{Return}_{Y,t} - \\bar{\\text{Return}}_{Y}) \\right) \\times 252 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447844a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = yf.download(tickers = \"^GSPC\", start = start_date, end = end_date)\n",
    "returns[\"MP\"] = sp500[\"Close\"].pct_change(periods = 1).dropna( ) \n",
    "cov_table = returns.cov( ) * 252 \n",
    "\n",
    "summary_MP = risk_return(returns[\"MP\"].to_frame( ))\n",
    "summary = pd.concat(objs = [summary, summary_MP], axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_summary = summary[\"Annual\"].copy( )\n",
    "ann_summary.columns = [\"Annual Return\", \"Annual Volatility\", \"Annual Sharpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_table.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880e500",
   "metadata": {},
   "source": [
    "### 4.2 Beta, Systematic Risk, and Idiosyncratic Risk \n",
    "<a id=\"subsection-42\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07092024",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left( \\text{Total Risk}_S \\right)^2 = \\left( \\text{Annualized } \\sigma_S \\right)^2\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\left( \\text{Systematic Risk}_S \\right)^2 = \\text{Cov}(\\text{Return}_{S}, \\text{Return}_M) \\times 252\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\left( \\text{Unsystematic Risk}_S \\right)^2 = \\left( \\text{Total Risk}_S \\right)^2 - \\left( \\text{Systematic Risk}_S \\right)^2\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Annualized } \\beta_S = \\frac{\\text{Annualized Cov}(\\text{Return}_{S}, \\text{Return}_M)}{\\text{Annualized } \\sigma^2_M}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_summary[\"Systematic Risk\"] = cov_table[\"MP\"].apply(np.sqrt) \n",
    "ann_summary[\"Unsystematic Risk\"] = (ann_summary[\"Annual Volatility\"] ** 2 - ann_summary[\"Systematic Risk\"] ** 2).apply(np.sqrt) \n",
    "ann_summary[\"Beta\"] = cov_table[\"MP\"].div(cov_table.loc[\"MP\" , \"MP\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d349bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_summary[[\"Beta\", \"Systematic Risk\", \"Unsystematic Risk\"]].style\\\n",
    "                                                             .format(\"{:.4f}\")\\\n",
    "                                                             .highlight_max(color = \"lightgreen\")\\\n",
    "                                                             .highlight_min(color = \"lightcoral\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_summary[[\"Systematic Risk\" , \"Unsystematic Risk\"]].mul(100).plot(kind = \"bar\", figsize = (12 , 8), fontsize = 10, \n",
    "                                                                     rot = 90, yticks = range(0, 66, 5), color = [\"orange\", \"red\"])  \n",
    "\n",
    "plt.ylabel(ylabel = \"Annualized Risk (%)\", fontsize = 12) \n",
    "plt.style.use(\"default\")   \n",
    "plt.show( ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3c7ebc",
   "metadata": {},
   "source": [
    "### 4.3 Rolling Beta \n",
    "<a id=\"subsection-43\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_beta(returns_df, window = 3) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    - window : int for the interval of the rolling window (months)\n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Plots the rolling beta for each asset (3-month by default)\n",
    "    \"\"\"\n",
    "    sp500 = yf.download(tickers = \"^GSPC\", start = start_date, end = end_date)\n",
    "    sp500_returns = sp500[\"Close\"].pct_change(periods = 1).dropna( )\n",
    "    sp500_monthly_returns = sp500_returns.resample(\"BM\").agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1)\n",
    "    \n",
    "    monthly_returns = returns_df.resample(\"BM\").agg(lambda daily_ret: (1 + daily_ret).prod( ) - 1)\n",
    "    \n",
    "    rolling_cov = monthly_returns.rolling(window).cov(sp500_monthly_returns) \n",
    "    rolling_var = sp500_monthly_returns.rolling(window).var( )\n",
    "    rolling_beta = rolling_cov.div(rolling_var, axis = 0) \n",
    "    \n",
    "    assets = returns_df.columns \n",
    "    n_assets = len(returns_df.columns)\n",
    "    fig, ax = plt.subplots(n_assets, 1, figsize = (12, n_assets * 4)) \n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "    \n",
    "    for i in range(n_assets) :\n",
    "        ax[i].plot(rolling_beta[assets[i]], color = colors[i], label = assets[i])\n",
    "        ax[i].set_ylabel(ylabel = \"Rolling Beta\", fontsize = 12)\n",
    "        ax[i].legend( )\n",
    "    \n",
    "    plt.style.use(\"default\") \n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d78e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_beta(returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_beta(returns[portfs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17a757",
   "metadata": {},
   "source": [
    "### 4.4 Expected Return and Alpha \n",
    "<a id=\"subsection-44\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187dd47b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Expected Return} = \\text{Risk-Free Rate} + \\beta \\times (\\text{Return}_M - \\text{Risk-Free Rate})\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\text{Alpha} = \\text{Realized Return} - \\text{Expected Return}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce851fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_summary[\"CAPM Return\"] = rf_rate + (ann_summary.loc[\"MP\" , \"Annual Return\"] - rf_rate) * ann_summary[\"Beta\"] \n",
    "ann_summary[\"Alpha\"] = ann_summary[\"Annual Return\"].sub(ann_summary[\"CAPM Return\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_summary[[\"Annual Return\", \"CAPM Return\", \"Alpha\"]].style\\\n",
    "                                                      .format(\"{:.4f}\")\\\n",
    "                                                      .highlight_max(color = \"lightgreen\")\\\n",
    "                                                      .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c743f2",
   "metadata": {},
   "source": [
    "### 4.5 Security Market Line \n",
    "<a id=\"subsection-45\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12 , 8))\n",
    "\n",
    "# plot the priceweighted portfolio\n",
    "plt.scatter(x = ann_summary.loc[\"PWP\" , \"Beta\"], y = ann_summary.loc[\"PWP\" , \"Annual Return\"], s = 30, c = \"blue\")\n",
    "\n",
    "plt.annotate(\"PWP\", size = 8, c = \"blue\", \n",
    "             xy = (ann_summary.loc[\"PWP\" , \"Beta\"] + 0.02, ann_summary.loc[\"PWP\" , \"Annual Return\"]))\n",
    "\n",
    "# plot the equallyweighted portfolio\n",
    "plt.scatter(x = ann_summary.loc[\"EWP\" , \"Beta\"], y = ann_summary.loc[\"EWP\" , \"Annual Return\"], s = 30, c = \"blue\")\n",
    "\n",
    "plt.annotate(\"EWP\", size = 8, c = \"blue\", \n",
    "             xy = (ann_summary.loc[\"EWP\" , \"Beta\"] - 0.035, ann_summary.loc[\"EWP\" , \"Annual Return\"] + 0.01))\n",
    "\n",
    "# plot the market-valueweighted portfolio\n",
    "plt.scatter(x = ann_summary.loc[\"MVWP\" , \"Beta\"], y = ann_summary.loc[\"MVWP\" , \"Annual Return\"], s = 30, c = \"blue\")\n",
    "\n",
    "plt.annotate(\"MVWP\", size = 8, c = \"blue\", \n",
    "             xy = (ann_summary.loc[\"MVWP\" , \"Beta\"] + 0.02, ann_summary.loc[\"MVWP\" , \"Annual Return\"]))\n",
    "\n",
    "# plot the market portfolio\n",
    "plt.scatter(x = ann_summary.loc[\"MP\", \"Beta\"], y = ann_summary.loc[\"MP\", \"Annual Return\"], s = 30, c = \"green\")\n",
    "\n",
    "plt.annotate(\"MP\", size = 10, c = \"green\", \n",
    "             xy = (ann_summary.loc[\"MP\" , \"Beta\"] - 0.015, ann_summary.loc[\"MP\" , \"Annual Return\"] + 0.01))\n",
    "\n",
    "# plot the minimum variance portfolio\n",
    "plt.scatter(x = ann_summary.loc[\"MVP\", \"Beta\"], y = ann_summary.loc[\"MVP\", \"Annual Return\"], s = 30, c = \"blue\")\n",
    "\n",
    "plt.annotate(\"MVP\", size = 8, c = \"blue\", \n",
    "             xy = (ann_summary.loc[\"MVP\" , \"Beta\"] - 0.02, ann_summary.loc[\"MVP\" , \"Annual Return\"] - 0.025))\n",
    "\n",
    "# plot the maximum sharpe portfolio \n",
    "plt.scatter(x = ann_summary.loc[\"MSP\", \"Beta\"], y = ann_summary.loc[\"MSP\", \"CAPM Return\"], c = \"fuchsia\", s = 30)\n",
    "\n",
    "plt.annotate(\"MSP\", size = 8, c = \"fuchsia\", \n",
    "             xy = (ann_summary.loc[\"MSP\" , \"Beta\"] - 0.025, ann_summary.loc[\"MSP\" , \"CAPM Return\"] + 0.01))\n",
    "\n",
    "# plot the inverse volatility portfolio \n",
    "plt.scatter(x = ann_summary.loc[\"IVP\", \"Beta\"], y = ann_summary.loc[\"IVP\", \"CAPM Return\"], c = \"blue\", s = 30)\n",
    "\n",
    "plt.annotate(\"IVP\", size = 8, c = \"blue\", \n",
    "             xy = (ann_summary.loc[\"IVP\" , \"Beta\"] - 0.025, ann_summary.loc[\"IVP\" , \"CAPM Return\"] + 0.01))\n",
    "\n",
    "# plot the individual stocks in the portfolio \n",
    "plt.scatter(x = ann_summary.loc[tickers , \"Beta\"], y = ann_summary.loc[tickers, \"Annual Return\"], s = 30, c = \"black\")\n",
    "\n",
    "for ticker in tickers : \n",
    "    plt.annotate(ticker, xy = (ann_summary.loc[ticker , \"Beta\"] - 0.03, ann_summary.loc[ticker , \"Annual Return\"] + 0.01), size = 8)  \n",
    "\n",
    "#  plot the riskfree asset and the security market line \n",
    "plt.scatter(x = pd.Series(data = 0).values, y = rf_rate, s = 30, c = \"green\")\n",
    "plt.annotate(\"risk-free asset\", size = 10, c = \"green\", xy = (pd.Series(data = 0).values - 0.04, rf_rate + 0.03))\n",
    "\n",
    "betas = np.array(range(0, 180, 10)) / 100 \n",
    "capm_returns = rf_rate + (ann_summary.loc[\"MP\" , \"Annual Return\"] - rf_rate) * betas\n",
    "plt.plot(betas, capm_returns, c = \"green\")\n",
    "\n",
    "plt.fill_between(betas, capm_returns, color = \"red\", alpha = 0.3)\n",
    "plt.fill_between(betas, 0.8, color = \"green\", alpha = 0.2)\n",
    "plt.annotate(\"SML\", size = 10, c = \"green\", xy = (0.4, 0.10))\n",
    "\n",
    "plt.xticks(ticks = np.array(range(0, 18)) / 10)\n",
    "plt.yticks(ticks = np.array(range(0, 81, 5)) / 100)\n",
    "plt.xlabel(xlabel = \"Annualized Beta\",  fontsize = 12)\n",
    "plt.ylabel(ylabel = \"Annualized Return\",  fontsize = 12) \n",
    "plt.grid( )\n",
    "plt.style.use(\"default\")  \n",
    "plt.show( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdfbf95",
   "metadata": {},
   "source": [
    "## 5. Farma-French 5-Factor Model \n",
    "<a id=\"section-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1977daf",
   "metadata": {},
   "source": [
    "### 5.1 Factor Data \n",
    "<a id=\"subsection-51\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_five_factor = web.DataReader(\"F-F_Research_Data_5_Factors_2x3_Daily\", \"famafrench\", start = start_date, end = end_date)[0]\n",
    "ff_five_factor = ff_five_factor.iloc[1: , :-1].copy( )\n",
    "ff_five_factor.columns = [\"Excess Market Return\", \"Small minus Big\", \"High minus Low\", \"Robust minus Weak\", \"Conservative Minus Aggressive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_five_factor.describe( ).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc277b",
   "metadata": {},
   "source": [
    "### 5.2 Rolling Correlation With Factors\n",
    "<a id=\"subsection-52\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca62e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_corr(returns_df, ff_factor_df, window = 21) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns of the asset \n",
    "    - ff_factor_df : DataFrame of daily factor data \n",
    "    - window : int for the interval of the rolling window (days)\n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Plots the rolling correlation between the asset and each factor (21 days by default)\n",
    "    \"\"\"\n",
    "    asset = returns_df.name\n",
    "    rolling_corr = returns_df.rolling(window).corr(ff_factor_df) \n",
    "    \n",
    "    rolling_corr.plot(figsize = (12, 15), fontsize = 10, subplots = True, sharex = False, xlabel = \"\", \n",
    "                      title = [f\"Rolling Correlation Between {asset} and FF Factors\", \"\", \"\", \"\", \"\"])\n",
    "    \n",
    "    plt.style.use(\"default\")  \n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568001cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_corr(returns[\"AAPL\"], ff_five_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_corr(returns[\"NVDA\"], ff_five_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_corr(returns[\"MVP\"], ff_five_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6351f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_corr(returns[\"MSP\"], ff_five_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c331fb8",
   "metadata": {},
   "source": [
    "### 5.3 Linear Regression \n",
    "<a id=\"subsection-53\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b96206",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{R}_t = \\beta_{0} + \\beta_{1} \\times (\\text{MKT}_t - \\text{RF}_t) + \\beta_{2} \\times \\text{SMB}_t + \\beta_{3} \\times \\text{HML}_t + \\beta_{4} \\times \\text{RMW}_t + \\beta_{5} \\times \\text{CMA}_t + \\epsilon_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def farma_french(returns_df, ff_factor_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    - ff_factor_df : DataFrame of daily factor data \n",
    "    \n",
    "    Returns :\n",
    "    - ff_returns_df : DataFrame of daily returns predicted by the French-Farma model \n",
    "    - ff_coefs_df : DataFrame of regression coefficients \n",
    "    \n",
    "    Function :\n",
    "    - Fits a linear regression model with factor data as predictor variables and returns as the target variable  \n",
    "    - Retrieves the regression coefficients of each factor for each asset \n",
    "    \"\"\"\n",
    "    X = ff_factor_df.copy( )\n",
    "    Y = returns_df.copy( )\n",
    "    \n",
    "    lin_reg = linear_model.LinearRegression(fit_intercept = True)\n",
    "    lin_reg.fit(X , Y)\n",
    "    Y_pred = lin_reg.predict(X) \n",
    "    \n",
    "    coefs = lin_reg.coef_\n",
    "    coefs_columns = [f\"Coef_{factor}\" for factor in ff_factor_df.columns] \n",
    "    \n",
    "    ff_returns_df = pd.DataFrame(Y_pred, index = returns_df.index, columns = returns_df.columns)\n",
    "    ff_coefs_df = pd.DataFrame(coefs, index = returns_df.columns, columns = coefs_columns)\n",
    "    return ff_returns_df, ff_coefs_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ccb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_returns = farma_french(returns, ff_five_factor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_returns.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d045a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_summary(returns_df, ff_factor_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - returns_df : DataFrame of daily returns \n",
    "    - ff_factor_df : DataFrame of daily factor data \n",
    "    \n",
    "    Returns :\n",
    "    - None  \n",
    "    \n",
    "    Function :\n",
    "    - Prints the ANOVA table and summary statistics of the fitted French-Farma regression model for each asset \n",
    "    \"\"\"\n",
    "    X = ff_factor_df.to_numpy( )\n",
    "    X = tools.add_constant(X)\n",
    "    \n",
    "    for asset in returns_df.columns :\n",
    "        Y = returns_df[asset].to_numpy( )\n",
    "        lin_reg = regression.linear_model.OLS(endog = Y, exog = X).fit( )\n",
    "        summary = lin_reg.summary( )\n",
    "        summary.tables[0].title = asset\n",
    "        print(summary)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c517c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_summary(returns[tickers], ff_five_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf05978",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_summary(returns[portfs + [\"MP\"]], ff_five_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc11e00",
   "metadata": {},
   "source": [
    "### 5.4 Regression Coefficients : Magnitude of Factors Driving Returns\n",
    "<a id=\"subsection-54\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_coefs = farma_french(returns, ff_five_factor)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25acc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_coefs.style\\\n",
    "        .format(\"{:.4f}\")\\\n",
    "        .highlight_max(color = \"lightgreen\")\\\n",
    "        .highlight_min(color = \"lightcoral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa27f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ff_coefs(ff_coefs_df) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - ff_coefs_df : DataFrame of regression coefficients of each factor for each asset \n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Plots bar charts of the regression coefficients of each factor for each asset \n",
    "    \"\"\"\n",
    "    ff_coefs_df.mul(100).plot(kind = \"bar\", figsize = (12 , 25), fontsize = 10, width = 0.4, rot = 0, subplots = True, \n",
    "                              sharex = False, xlabel = \"\", ylabel = \"% Change in Return\", legend = False) \n",
    "\n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb10be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ff_coefs(ff_coefs.loc[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ff_coefs(ff_coefs.loc[portfs + [\"MP\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e6a86",
   "metadata": {},
   "source": [
    "### 5.5 Model Evaluation and Residual Analysis\n",
    "<a id=\"subsection-55\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e5d68",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Mean Squared Error} = \\frac{1}{T} \\sum_{t=1}^{T} (\\text{Return}_t - \\hat{\\text{Return}}_t)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Mean Absolute Error} = \\frac{1}{T} \\sum_{t=1}^{T} |\\text{Return}_t - \\hat{\\text{Return}}_t|\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\text{Mean Absolute Percentage Error} = \\frac{1}{T} \\sum_{t=1}^{T} \\left| \\frac{\\text{Return}_t - \\hat{\\text{Return}}_t}{\\text{Return}_t} \\right| \\times 100\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{R}^2 = 1 - \\frac{{\\sum_{t=1}^{T} (\\text{Return}_t - \\hat{\\text{Return}}_t)^2}}{{\\sum_{t=1}^{T} (\\text{Return}_t - \\bar{\\text{Return}})^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Adjusted R}^2 = 1 - \\frac{{(1 - R^2) \\times (T - 1)}}{{T - 5 - 1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Akaike Information Criterion} = T \\times \\log\\left(\\frac{SSE}{T}\\right) + 2 \\times 5\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Bayesian Information Criterion} = T \\times \\log\\left(\\frac{SSE}{T}\\right) + 5 \\times \\log(T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perf(Y, Y_pred) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - Y : DataFrame of actual daily returns \n",
    "    - Y_pred : DataFrame of daily returns predicted by the French-Farma model \n",
    "    \n",
    "    Returns :\n",
    "    - perf_df : DataFrame displaying ratios for evaluating a regression model \n",
    "    \n",
    "    Function :\n",
    "    - Computes the mean squared error, mean absolute error, mean absolute percentage error, r-squared, adjusted r-squared, akaike information criterion, and bayesian information criterion \n",
    "    \"\"\"\n",
    "    perf_metrics = {  }\n",
    "    for asset in Y.columns :\n",
    "        \n",
    "        mse = metrics.mean_squared_error(Y[asset], Y_pred[asset])\n",
    "        mae = metrics.mean_absolute_error(Y[asset], Y_pred[asset]) \n",
    "        mape = metrics.mean_absolute_percentage_error(Y[asset], Y_pred[asset]) \n",
    "        r2 = metrics.r2_score(Y[asset], Y_pred[asset])\n",
    "        adjusted_r2 = 1 - ((1 - r2) * (len(Y) - 1) / (len(Y) - 5 - 1))\n",
    "        \n",
    "        residuals = Y[asset] - Y_pred[asset] \n",
    "        sse = np.sum(residuals ** 2)\n",
    "        aic = len(Y[asset]) * np.log(sse / len(Y[asset])) + 2 * 5   \n",
    "        bic = len(Y[asset]) * np.log(sse / len(Y[asset])) + 5 * np.log(len(Y[asset])) \n",
    "        \n",
    "        perf_metrics[asset] = [mse, mae, mape, r2, adjusted_r2, aic, bic] \n",
    "    \n",
    "    perf_metrics_df = pd.DataFrame(perf_metrics, index = [\"Mean Squared Error\", \"Mean Absolute Error\", \"Mean Absolute Percentage Error\", \n",
    "                                                          \"R-Squared\", \"Adjusted R-Squared\", \n",
    "                                                          \"Akaike Information Criterion\", \"Bayesian Information Criterion\"]).T    \n",
    "    return perf_metrics_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_model_perf = eval_perf(returns, ff_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_model_perf.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8830ce",
   "metadata": {},
   "source": [
    "$$\n",
    "e_t = y_t - \\hat{y}_t \n",
    "$$ \n",
    "\n",
    "$\\text{Where :}$\n",
    "- $ y_t \\text{ represents the actual return at time \\( t \\).} $\n",
    "- $ \\hat{y}_t \\text{ represents the predicted return at time \\( t \\).} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127174e7",
   "metadata": {},
   "source": [
    "$$\n",
    "DW = \\frac{\\sum_{t=2}^{T} (e_t - e_{t-1})^2}{\\sum_{t=1}^{T} e_t^2} \n",
    "$$\n",
    "\n",
    "$\\text{Where :}$\n",
    "- $ e_t \\text{ represents the residual at time \\( t \\).} $\n",
    "- $ T \\text{ is the number of periods.} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb72274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_summary(Y, Y_pred) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - Y : DataFrame of actual daily returns \n",
    "    - Y_pred : DataFrame of daily returns predicted by French-Farma model \n",
    "    \n",
    "    Returns :\n",
    "    - None     \n",
    "    \n",
    "    Function :\n",
    "     - Computes the difference between actual daily returns and predicted daily returns \n",
    "    - Prints the average, standard deviation, minimum, 25th percentile, median, 75th percentile, and maximum of residuals \n",
    "    \"\"\"\n",
    "    residuals = Y - Y_pred \n",
    "    summary_df = residuals.mul(100).describe( ).T.drop(columns = \"count\")\n",
    "    summary_df.columns = [\"Average\", \"Standard Deviation\", \"Minimum\", \"25th Percentile\", \"Median\", \"75th Percentile\", \"Maximum\"]\n",
    "    summary_df.columns = [f\"{col} (%)\" for col in summary_df.columns]\n",
    "    \n",
    "    print(\"Summary Statistics of French-Farma Regression Residuals (%)\")\n",
    "    return summary_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_summary(returns, ff_returns).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206cee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(Y, Y_pred) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - Y : DataFrame of actual daily returns \n",
    "    - Y_pred : DataFrame of daily returns predicted by French-Farma model \n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Computes the difference between actual daily returns and predicted daily returns \n",
    "    - Plots line graphs of residuals over time for each asset \n",
    "    - Plots scatter points for the 10 largest positive residuals and 10 largest negative residuals \n",
    "    \"\"\"\n",
    "    residuals = Y - Y_pred \n",
    "    \n",
    "    n_assets = len(Y.columns)\n",
    "    fig , ax = plt.subplots(figsize = (12 , n_assets * 5), sharex = False, sharey = False, nrows = n_assets, ncols = 1)\n",
    "    \n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "    for i, asset in enumerate(Y.columns) :\n",
    "        \n",
    "        ax[i].plot(residuals[asset] * 100, color = colors[i], label = asset)\n",
    "        dw_statistic = stattools.durbin_watson(residuals[asset])\n",
    "        legend_label = f\"{asset}\\nDurbin-Watson Statistic = {dw_statistic:.2f}\"\n",
    "        \n",
    "        largest_positive_resids = residuals[asset].nlargest(10).sort_index( ) \n",
    "        largest_negative_resids = residuals[asset].nsmallest(10).sort_index( ) \n",
    "        ax[i].scatter(largest_positive_resids.index, largest_positive_resids.values * 100, color = colors[i], s = 30)\n",
    "        ax[i].scatter(largest_negative_resids.index, largest_negative_resids.values * 100, color = colors[i], s = 30)\n",
    "        \n",
    "        ax[i].set_ylabel(ylabel = \"Residual (%)\", fontsize = 12)\n",
    "        ax[i].legend([legend_label], fontsize = 10)\n",
    "    \n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c642ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(returns[tickers], ff_returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(returns[portfs + [\"MP\"]], ff_returns[portfs + [\"MP\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fitted_vs_actual(Y, Y_pred, cumulative = False) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - Y : DataFrame of actual daily returns \n",
    "    - Y_pred : DataFrame of daily returns predicted by French-Farma model \n",
    "    - cumulative : boolean indicating whether to compute cumulative returns (False by default)\n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Plots the line graph of predicted daily returns and scatter points of actual daily returns for each asset \n",
    "    - Plots the ligne graphs of predicted cumulative returns and actual cumulative returns for each asset \n",
    "    \"\"\"\n",
    "    residuals = Y - Y_pred \n",
    "    \n",
    "    n_assets = len(Y.columns)\n",
    "    fig , ax = plt.subplots(figsize = (12 , n_assets * 5), sharex = False, sharey = False, nrows = n_assets, ncols = 1)\n",
    "    \n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "    for i, asset in enumerate(Y.columns) :\n",
    "        \n",
    "        if cumulative == False :\n",
    "            ax[i].plot(Y_pred[asset] * 100, color = colors[i], label = f\"{asset} Fitted\")\n",
    "            ax[i].scatter(Y.index, Y[asset] * 100, color = \"black\", alpha = 0.3, label = \"Actual\")\n",
    "        \n",
    "            ax[i].set_ylabel(ylabel = \"Daily Return (%)\", fontsize = 12)\n",
    "            ax[i].legend( )\n",
    "        \n",
    "        else :\n",
    "            cum_returns_asset = (1 + Y[asset]).cumprod( ) - 1 \n",
    "            cum_returns_ff = (1 + Y_pred[asset]).cumprod( ) - 1 \n",
    "        \n",
    "            ax[i].plot(cum_returns_asset * 100, color = colors[i], label = asset)\n",
    "            ax[i].plot(cum_returns_ff * 100, color = \"black\", label = \"Fitted\")\n",
    "        \n",
    "            ax[i].set_ylabel(ylabel = \"Cumulative Returns (%)\", fontsize = 12)\n",
    "            ax[i].legend( )\n",
    "        \n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fitted_vs_actual(returns[tickers], ff_returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec01647",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fitted_vs_actual(returns[tickers], ff_returns[tickers], cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fitted_vs_actual(returns[portfs + [\"MP\"]], ff_returns[portfs + [\"MP\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fitted_vs_actual(returns[portfs + [\"MP\"]], ff_returns[portfs + [\"MP\"]], cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33302f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals_vs_fitted(Y, Y_pred) :\n",
    "    \"\"\"\n",
    "    Arguments :\n",
    "    - Y : DataFrame of actual daily returns \n",
    "    - Y_pred : DataFrame of daily returns predicted by French-Farma model \n",
    "    \n",
    "    Returns :\n",
    "    - None \n",
    "    \n",
    "    Function :\n",
    "    - Draws a scatter plot with predited daily returns on the x-axis and residuals on the y-axis for each asset \n",
    "    - Draws 2 horizontal lines for the 95% confidence interval of residual values for each asset \n",
    "    \"\"\"\n",
    "    residuals = Y - Y_pred \n",
    "    \n",
    "    n_assets = len(Y.columns)\n",
    "    fig , ax = plt.subplots(figsize = (12 , n_assets * 5), sharex = False, sharey = False, nrows = n_assets, ncols = 1)\n",
    "    \n",
    "    colors = sns.color_palette(\"tab10\") + [\"limegreen\", \"fuchsia\"]\n",
    "    for i, asset in enumerate(Y.columns) :\n",
    "        \n",
    "        ax[i].scatter(Y_pred[asset] * 100, residuals[asset] * 100, color = colors[i], label = asset)\n",
    "        \n",
    "        ax[i].axhline(y = 0, color = \"black\", linestyle = \"--\")\n",
    "        ax[i].axhline(y = (residuals[asset].mean( ) - 2 * residuals[asset].std( )) * 100, \n",
    "                      color = \"red\", linestyle = \"--\", label = \" - 2 (5%)\")\n",
    "        ax[i].axhline(y = (residuals[asset].mean( ) + 2 * residuals[asset].std( )) * 100, \n",
    "                      color = \"green\", linestyle = \"--\", label = \" + 2 (95%)\")\n",
    "        \n",
    "        ax[i].set_xlabel(xlabel = \"Fitted Return (%)\", fontsize = 12)\n",
    "        ax[i].set_ylabel(ylabel = \"Residual (%)\", fontsize = 12)\n",
    "        ax[i].legend( )\n",
    "        \n",
    "    plt.style.use(\"default\")\n",
    "    plt.show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_fitted(returns[tickers], ff_returns[tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4793ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_vs_fitted(returns[portfs + [\"MP\"]], ff_returns[portfs + [\"MP\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad486c",
   "metadata": {},
   "source": [
    "## Thank You for Your Attention. =))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b069ca",
   "metadata": {},
   "source": [
    "# Author \n",
    "[Khai Lap Vuong](https://www.linkedin.com/in/khai-lap-vuong/)\n",
    "\n",
    "## Other Contributor(s) \n",
    "[Ruslan Goyenko](https://www.linkedin.com/in/russ-goyenko-24a5777/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
